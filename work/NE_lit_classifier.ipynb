{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc527373-5cb8-4636-bbac-66e0a7fc3bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.11/site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2be3fbb-0e7a-44c2-8807-381e580db097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 09:14:23.178368: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-12 09:14:23.618430: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-12 09:14:25.215902: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-12 09:14:28.459803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import aiohttp\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "14aaca76-8909-41ed-8bc8-1a8490882f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnAnalysis:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.entity_type_dict = {\n",
    "            \"PERSON\": \"NE\",\n",
    "            \"NORP\": \"NE\",\n",
    "            \"FAC\": \"NE\",\n",
    "            \"ORG\": \"NE\",\n",
    "            \"GPE\": \"NE\",\n",
    "            \"LOC\": \"NE\",\n",
    "            \"PRODUCT\": \"NE\",\n",
    "            \"EVENT\": \"NE\",\n",
    "            \"WORK_OF_ART\": \"NE\",\n",
    "            \"LAW\": \"NE\",\n",
    "            \"LANGUAGE\": \"NE\",\n",
    "            \"DATE\": \"LIT\",\n",
    "            \"TIME\": \"LIT\",\n",
    "            \"PERCENT\": \"LIT\",\n",
    "            \"MONEY\": \"LIT\",\n",
    "            \"QUANTITY\": \"LIT\",\n",
    "            \"ORDINAL\": \"LIT\",\n",
    "            \"CARDINAL\": \"LIT\",\n",
    "            \"URL\": \"LIT\",\n",
    "            \"DESC\": \"LIT\",\n",
    "            \"TOKEN\": \"NE\",\n",
    "            \"INTEGER\": \"LIT\",\n",
    "            \"FLOAT\": \"LIT\",\n",
    "            \"DATETIME\": \"LIT\",\n",
    "            \"ADDRESS\": \"LIT\",\n",
    "            \"EMAIL\": \"LIT\"\n",
    "        }\n",
    "\n",
    "        self.LIT_DATATYPE = {\n",
    "            \"DATE\": \"DATETIME\", \n",
    "            \"TIME\": \"STRING\", \n",
    "            \"PERCENT\": \"STRING\", \n",
    "            \"MONEY\": \"STRING\", \n",
    "            \"QUANTITY\": \"STRING\", \n",
    "            \"ORDINAL\": \"NUMBER\", \n",
    "            \"CARDINAL\": \"NUMBER\", \n",
    "            \"URL\": \"STRING\",\n",
    "            \"DESC\": \"STRING\",\n",
    "            \"TOKEN\": \"STRING\",\n",
    "            \"INTEGER\": \"NUMBER\",\n",
    "            \"FLOAT\": \"NUMBER\",\n",
    "            \"DATETIME\": \"DATETIME\",\n",
    "            \"ADDRESS\": \"STRING\",\n",
    "            \"EMAIL\": \"STRING\",\n",
    "            \"STRING\": \"STRING\"\n",
    "        }\n",
    "\n",
    "        self.NE_DATATYPE = [\"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\"]\n",
    "    \n",
    "    def most_frequent_element(self, input_list):\n",
    "        counter = Counter(input_list)\n",
    "        most_common = counter.most_common(1)\n",
    "        return most_common[0][0] if most_common else None\n",
    "\n",
    "    def extract_number_features(self, column):\n",
    "        try:\n",
    "            col = pd.to_numeric(column, errors='coerce')\n",
    "            return {\n",
    "                'min_value': np.min(col),\n",
    "                'max_value': np.max(col),\n",
    "                'mean_value': np.mean(col),\n",
    "                'std_dev': np.std(col),\n",
    "                'unique_count': len(set(col))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting number features: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def extract_named_entity_features(self, column):\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'all_caps': sum(1 for entry in column if str(entry).isupper()),\n",
    "            'capitalized': sum(1 for entry in column if str(entry).istitle()),\n",
    "            'hyphens': sum(str(entry).count('-') for entry in column),\n",
    "            'periods': sum(str(entry).count('.') for entry in column),\n",
    "            'commas': sum(str(entry).count(',') for entry in column)\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_string_features(self, column):\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'all_caps': sum(1 for entry in column if str(entry).isupper()),\n",
    "            'capitalized': sum(1 for entry in column if str(entry).istitle()),\n",
    "            'alphabetic_chars': sum(char.isalpha() for entry in column for char in str(entry)),\n",
    "            'digit_chars': sum(char.isdigit() for entry in column for char in str(entry)),\n",
    "            'special_chars': sum(not char.isalnum() for entry in column for char in str(entry))\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_datetime_features(self, column):\n",
    "        dates = pd.to_datetime(column, errors='coerce')\n",
    "        features = {\n",
    "            'min_date': dates.min(),\n",
    "            'max_date': dates.max(),\n",
    "            'year_counts': dates.dt.year.value_counts().to_dict(),\n",
    "            'month_counts': dates.dt.month.value_counts().to_dict()\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_url_features(self, column):\n",
    "        url_pattern = re.compile(r'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$', re.IGNORECASE)\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'valid_urls': sum(1 for entry in column if re.match(url_pattern, str(entry)))\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_address_features(self, column):\n",
    "        address_pattern = re.compile(r'\\d+\\s+\\w+\\s+(?:street|st|avenue|ave|road|rd|boulevard|blvd|lane|ln|drive|dr|court|ct|circle|cir|place|pl)\\.?\\s*\\w*', re.IGNORECASE)\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'address_count': sum(1 for entry in column if re.match(address_pattern, str(entry)))\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_email_features(self, column):\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', re.IGNORECASE)\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'valid_emails': sum(1 for entry in column if re.match(email_pattern, str(entry)))\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    async def fetch_entity(self, session, cell):\n",
    "        if cell is None or pd.isna(cell):\n",
    "            return None\n",
    "        cell = str(cell)\n",
    "        url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "        params = {\n",
    "            'name': cell,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            #'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{cell}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "            #'sort': [\n",
    "            #    f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            #]\n",
    "        }\n",
    "        async with session.get(url, params=params, ssl=False, timeout=100) as response:\n",
    "            if response.status == 200:\n",
    "                return await response.json()\n",
    "            return None\n",
    "\n",
    "    async def classify_columns_async(self, df):\n",
    "        def combine_scores(j_score, ed_score, w1=0.5, w2=0.5):\n",
    "            return w1 * j_score + w2 * ed_score\n",
    "\n",
    "        url_pattern = re.compile(r'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$', re.IGNORECASE)\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', re.IGNORECASE)\n",
    "        address_pattern = re.compile(r'\\d+\\s+\\w+\\s+(?:street|st|avenue|ave|road|rd|boulevard|blvd|lane|ln|drive|dr|court|ct|circle|cir|place|pl)\\.?\\s*\\w*', re.IGNORECASE)\n",
    "        datetime_pattern = re.compile(\n",
    "            r'(?:\\d{4}-\\d{2}-\\d{2})'  # YYYY-MM-DD format\n",
    "            r'|(?:31(?:\\/|-|\\.)0?[13578]|1[02](?:\\/|-|\\.)\\d{4})'  # 31 days months\n",
    "            r'|(?:29|30(?:\\/|-|\\.)0?[1,3-9]|1[0-2](?:\\/|-|\\.)\\d{4})'  # 29/30 days months\n",
    "            r'|(?:0?[1-9]|[12]\\d|3[01])(?:\\/|-|\\.)'  # Day\n",
    "            r'(?:0?[1-9]|1[0-2])(?:\\/|-|\\.)\\d{4}'  # Month\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/(?:\\d{2})'  # MM/DD/YY format\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/\\d{2}'  # MM/DD/YY format\n",
    "            r'\\b\\d{2}/(?:0?[1-9]|[12]\\d|3[01])/(?:0?[1-9]|1[0-2])\\b'  # YY/DD/MM format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d\\.[0-5]\\d'  # HH:MM.SS format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d'  # HH:MM format\n",
    "            r'|(?:[0-5]?\\d):[0-5]\\d(?:\\.\\d{1,2})?'  # H:MM or H:MM.S format\n",
    "            r'|(?:2[0-3]|[01]?\\d)h[0-5]?\\d(?:m[0-5]?\\d(?:\\.\\d{1,2})?s)?',  # HhMMmSSs format\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        col_type = []\n",
    "        feature_list = []\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for col_name, col_data in df.items():\n",
    "                type = []\n",
    "                count_cell = 0\n",
    "\n",
    "                for cell in col_data:\n",
    "                    label = None\n",
    "                    is_number = False\n",
    "    \n",
    "                    try:\n",
    "                        if math.isnan(cell):\n",
    "                            label = \"None\"\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "                    if isinstance(cell, str):\n",
    "                        if cell == \"NaN\" or cell == \"nan\":\n",
    "                            label = \"None\"\n",
    "                        elif re.match(url_pattern, cell):\n",
    "                            label = \"URL\"\n",
    "                        elif re.match(email_pattern, cell):\n",
    "                            label = \"EMAIL\"\n",
    "                        elif re.match(address_pattern, cell):\n",
    "                            label = \"ADDRESS\"\n",
    "                        elif re.match(datetime_pattern, cell):\n",
    "                            label = \"DATETIME\"\n",
    "                    \n",
    "                    if label is None:  # if it's none of the types below\n",
    "                        try:\n",
    "                            cell_str = str(cell)\n",
    "                            if ',' in cell_str or '.' in cell_str or '%' in cell_str or '$' in cell_str:\n",
    "                                cell_str = cell_str.replace('.', '').replace(',', '').replace('%', '').replace('$', '')\n",
    "                            if len(cell_str) - len(re.findall(r'\\d', cell_str)) < 5 and len(re.findall(r'\\d', cell_str)) != 0:\n",
    "                                is_number = True\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    if is_number:\n",
    "                        label = \"NUMBER\"\n",
    "                    elif isinstance(cell, bool):\n",
    "                        label = \"STRING\"\n",
    "                    elif label != \"None\" and len(cell.split(\" \")) >= 15:\n",
    "                        label = \"NOA\"\n",
    "                    elif label != \"None\" and len(cell.split(\" \")) >= 1 and len(cell) <= 4:\n",
    "                        label = \"STRING\"\n",
    "                    \n",
    "                    if label is not None:\n",
    "                        type.append(label)\n",
    "                    else:\n",
    "                        if count_cell > 5:\n",
    "                            type.append(\"STRING\")\n",
    "                            break  \n",
    "                        else:                \n",
    "                            tasks = [self.fetch_entity(session, cell) for cell in col_data if cell is not None and count_cell <= 5]\n",
    "                            responses = await asyncio.gather(*tasks)\n",
    "                            \n",
    "                            for cell, data in zip(col_data, responses):\n",
    "                                #print(f\"{cell}-->{data}\")\n",
    "                                if data and len(data) > 0 and data[0]['NERtype'] != None:\n",
    "                                    if combine_scores(data[0]['jaccard_score'], data[0]['ed_score']) >= 0.7:\n",
    "                                        type.append(f\"NE_{data[0]['NERtype']}\")\n",
    "                                else:\n",
    "                                    # if you didn't find a NER type for this i2tem\n",
    "                                    type.append(\"STRING\")\n",
    "                                count_cell += 1\n",
    "\n",
    "                most_common_type = self.most_frequent_element(type)\n",
    "                col_type.append(most_common_type)\n",
    "\n",
    "                if most_common_type == \"NUMBER\":\n",
    "                    features = self.extract_number_features(col_data)\n",
    "                elif most_common_type in ['NE_PERS', 'NE_LOC', 'NE_ORG', 'NE_OTHERS']:\n",
    "                    features = self.extract_named_entity_features(col_data)\n",
    "                elif most_common_type == \"STRING\" or most_common_type == \"NOA\":\n",
    "                    features = self.extract_string_features(col_data)\n",
    "                elif most_common_type == \"DATETIME\":\n",
    "                    features = self.extract_datetime_features(col_data)\n",
    "                elif most_common_type == \"URL\":\n",
    "                    features = self.extract_url_features(col_data)\n",
    "                elif most_common_type == \"ADDRESS\":\n",
    "                    features = self.extract_address_features(col_data)\n",
    "                elif most_common_type == \"EMAIL\":\n",
    "                    features = self.extract_email_features(col_data)\n",
    "                else:\n",
    "                    features = {}\n",
    "\n",
    "                features['column_name'] = col_name\n",
    "                features['column_type'] = most_common_type\n",
    "                feature_list.append(features)\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "    def classify_columns(self, df):\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(self.classify_columns_async(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "92aca4ec-d360-4e4e-98eb-26e0a9e4b909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2161 [00:00<?, ?it/s]/tmp/ipykernel_138/2242469751.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_df = pd.concat([train_df, pd.DataFrame(row)], ignore_index=True)\n",
      "/tmp/ipykernel_138/2242469751.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_df = pd.concat([train_df, pd.DataFrame(row)], ignore_index=True)\n",
      "  0%|          | 1/2161 [00:31<18:39:21, 31.09s/it]\n"
     ]
    },
    {
     "ename": "ClientConnectorError",
     "evalue": "Cannot connect to host lamapi.hel.sintef.cloud:443 ssl:False [Connect call failed ('65.108.140.148', 443)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:980\u001b[0m, in \u001b[0;36mTCPConnector._wrap_create_connection\u001b[0;34m(self, req, timeout, client_error, *args, **kwargs)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ceil_timeout(timeout\u001b[38;5;241m.\u001b[39msock_connect):\n\u001b[0;32m--> 980\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_connection(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[return-value]  # noqa\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m cert_errors \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/base_events.py:1085\u001b[0m, in \u001b[0;36mBaseEventLoop.create_connection\u001b[0;34m(self, protocol_factory, host, port, ssl, family, proto, flags, sock, local_addr, server_hostname, ssl_handshake_timeout, ssl_shutdown_timeout, happy_eyeballs_delay, interleave)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exceptions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;66;03m# If they all have the same str(), raise one.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/base_events.py:1069\u001b[0m, in \u001b[0;36mBaseEventLoop.create_connection\u001b[0;34m(self, protocol_factory, host, port, ssl, family, proto, flags, sock, local_addr, server_hostname, ssl_handshake_timeout, ssl_shutdown_timeout, happy_eyeballs_delay, interleave)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1069\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_sock(\n\u001b[1;32m   1070\u001b[0m         exceptions, addrinfo, laddr_infos)\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/base_events.py:973\u001b[0m, in \u001b[0;36mBaseEventLoop._connect_sock\u001b[0;34m(self, exceptions, addr_info, local_addr_infos)\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno matching local address with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfamily\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 973\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock_connect(sock, address)\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/selector_events.py:634\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop.sock_connect\u001b[0;34m(self, sock, address)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# Needed to break cycles when an exception occurs.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/selector_events.py:674\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop._sock_connect_cb\u001b[0;34m(self, fut, sock, address)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    673\u001b[0m         \u001b[38;5;66;03m# Jump to any except clause below.\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(err, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnect call failed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maddress\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mBlockingIOError\u001b[39;00m, \u001b[38;5;167;01mInterruptedError\u001b[39;00m):\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# socket is still registered, the callback will be retried later\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connect call failed ('65.108.140.148', 443)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mClientConnectorError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#tables_path = \"./data/Dataset/Dataset/2T_Round4/tables/\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#tables_path = \"./data/Dataset/Dataset/Round4_2020/tables/\"    \u001b[39;00m\n\u001b[1;32m     40\u001b[0m     tables_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/Dataset/Dataset/Round3_2019/tables/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 41\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m (main(tables_path))\n",
      "Cell \u001b[0;32mIn[157], line 33\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(tables_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Randomly select 5000 tables from table_files\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#table_files = random.sample(table_files, 5000)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_file \u001b[38;5;129;01min\u001b[39;00m tqdm(table_files):\n\u001b[0;32m---> 33\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_table(column_analysis, table_file, train_df, columns)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_df\n",
      "Cell \u001b[0;32mIn[157], line 3\u001b[0m, in \u001b[0;36mprocess_table\u001b[0;34m(column_analysis, table_path, train_df, columns)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_table\u001b[39m(column_analysis, table_path, train_df, columns):\n\u001b[1;32m      2\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(table_path)\n\u001b[0;32m----> 3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m column_analysis\u001b[38;5;241m.\u001b[39mclassify_columns_async(df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m      6\u001b[0m         row \u001b[38;5;241m=\u001b[39m {col: [entry\u001b[38;5;241m.\u001b[39mget(col, \u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns}\n",
      "Cell \u001b[0;32mIn[133], line 244\u001b[0m, in \u001b[0;36mColumnAnalysis.classify_columns_async\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:                \n\u001b[1;32m    243\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_entity(session, cell) \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m col_data \u001b[38;5;28;01mif\u001b[39;00m cell \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m count_cell \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m--> 244\u001b[0m     responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cell, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(col_data, responses):\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;66;03m#print(f\"{cell}-->{data}\")\u001b[39;00m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNERtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[133], line 159\u001b[0m, in \u001b[0;36mColumnAnalysis.fetch_entity\u001b[0;34m(self, session, cell)\u001b[0m\n\u001b[1;32m    148\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://lamapi.hel.sintef.cloud/lookup/entity-retrieval\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    149\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: cell,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlamapi_demo_2023\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     ]\n\u001b[1;32m    158\u001b[0m }\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams, ssl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client.py:1167\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client.py:562\u001b[0m, in \u001b[0;36mClientSession._request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ceil_timeout(real_timeout\u001b[38;5;241m.\u001b[39mconnect):\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m         conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[1;32m    563\u001b[0m             req, traces\u001b[38;5;241m=\u001b[39mtraces, timeout\u001b[38;5;241m=\u001b[39mreal_timeout\n\u001b[1;32m    564\u001b[0m         )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerTimeoutError(\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection timeout \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto host \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url)\n\u001b[1;32m    568\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:540\u001b[0m, in \u001b[0;36mBaseConnector.connect\u001b[0;34m(self, req, traces, timeout)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m trace\u001b[38;5;241m.\u001b[39msend_connection_create_start()\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection(req, traces, timeout)\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m    542\u001b[0m         proto\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:901\u001b[0m, in \u001b[0;36mTCPConnector._create_connection\u001b[0;34m(self, req, traces, timeout)\u001b[0m\n\u001b[1;32m    899\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_proxy_connection(req, traces, timeout)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_direct_connection(req, traces, timeout)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proto\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:1209\u001b[0m, in \u001b[0;36mTCPConnector._create_direct_connection\u001b[0;34m(self, req, traces, timeout, client_error)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m last_exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m last_exc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:1178\u001b[0m, in \u001b[0;36mTCPConnector._create_direct_connection\u001b[0;34m(self, req, traces, timeout, client_error)\u001b[0m\n\u001b[1;32m   1175\u001b[0m port \u001b[38;5;241m=\u001b[39m hinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     transp, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_create_connection(\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_factory,\n\u001b[1;32m   1180\u001b[0m         host,\n\u001b[1;32m   1181\u001b[0m         port,\n\u001b[1;32m   1182\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1183\u001b[0m         ssl\u001b[38;5;241m=\u001b[39msslcontext,\n\u001b[1;32m   1184\u001b[0m         family\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1185\u001b[0m         proto\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproto\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1186\u001b[0m         flags\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1187\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhostname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m sslcontext \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1188\u001b[0m         local_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_addr,\n\u001b[1;32m   1189\u001b[0m         req\u001b[38;5;241m=\u001b[39mreq,\n\u001b[1;32m   1190\u001b[0m         client_error\u001b[38;5;241m=\u001b[39mclient_error,\n\u001b[1;32m   1191\u001b[0m     )\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientConnectorError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1193\u001b[0m     last_exc \u001b[38;5;241m=\u001b[39m exc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:988\u001b[0m, in \u001b[0;36mTCPConnector._wrap_create_connection\u001b[0;34m(self, req, timeout, client_error, *args, **kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 988\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m client_error(req\u001b[38;5;241m.\u001b[39mconnection_key, exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mClientConnectorError\u001b[0m: Cannot connect to host lamapi.hel.sintef.cloud:443 ssl:False [Connect call failed ('65.108.140.148', 443)]"
     ]
    }
   ],
   "source": [
    "async def process_table(column_analysis, table_path, train_df, columns):\n",
    "    df = pd.read_csv(table_path)\n",
    "    result = await column_analysis.classify_columns_async(df.iloc[1:10])\n",
    "\n",
    "    for entry in result:\n",
    "        row = {col: [entry.get(col, None)] for col in columns}\n",
    "        train_df = pd.concat([train_df, pd.DataFrame(row)], ignore_index=True)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "async def main(tables_path):\n",
    "    column_analysis = ColumnAnalysis()\n",
    "\n",
    "    columns = [\n",
    "        'column_name', 'column_type', 'min_value', 'max_value', 'mean_value', 'std_dev', 'unique_count', 'special_values',\n",
    "        'average_length', 'min_length', 'max_length', 'all_caps', 'capitalized', 'hyphens', 'periods', 'commas', 'common_prefixes', 'common_suffixes',\n",
    "        'alphabetic_chars', 'digit_chars', 'special_chars', 'min_date', 'max_date', 'date_range', 'year_counts', 'month_counts',\n",
    "        'valid_urls', 'address_count', 'valid_emails'\n",
    "    ]\n",
    "    \n",
    "    train_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    pattern = r'^\\.'\n",
    "\n",
    "    # Create a list of file paths, excluding files that start with a dot\n",
    "    table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path) if not re.match(pattern, table)]\n",
    "\n",
    "    # Randomly select 5000 tables from table_files\n",
    "    #table_files = random.sample(table_files, 5000)\n",
    "\n",
    "\n",
    "    for table_file in tqdm(table_files):\n",
    "        train_df = await process_table(column_analysis, table_file, train_df, columns)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #tables_path = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "    #tables_path = \"./data/Dataset/Dataset/Round4_2020/tables/\"    \n",
    "    tables_path = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "    train_df = await (main(tables_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f49f51fe-3752-4057-8ebc-09e395963c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = \"./2T_train_df.csv\"\n",
    "train_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6394a2f1-3621-4704-a260-c2c7bc5cdb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = \"./R4_train_df.csv\"\n",
    "train_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360ce97-cf4c-45b7-95aa-9abb95c28564",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = \"./HT2_train_df.csv\"\n",
    "train_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3d64a57-c1e8-46fd-a2b3-e47734fbc098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ClientConnectorError",
     "evalue": "Cannot connect to host lamapi.hel.sintef.cloud:443 ssl:default [Connect call failed ('65.108.140.148', 443)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:980\u001b[0m, in \u001b[0;36mTCPConnector._wrap_create_connection\u001b[0;34m(self, req, timeout, client_error, *args, **kwargs)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ceil_timeout(timeout\u001b[38;5;241m.\u001b[39msock_connect):\n\u001b[0;32m--> 980\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_connection(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[return-value]  # noqa\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m cert_errors \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/base_events.py:1085\u001b[0m, in \u001b[0;36mBaseEventLoop.create_connection\u001b[0;34m(self, protocol_factory, host, port, ssl, family, proto, flags, sock, local_addr, server_hostname, ssl_handshake_timeout, ssl_shutdown_timeout, happy_eyeballs_delay, interleave)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exceptions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;66;03m# If they all have the same str(), raise one.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/base_events.py:1069\u001b[0m, in \u001b[0;36mBaseEventLoop.create_connection\u001b[0;34m(self, protocol_factory, host, port, ssl, family, proto, flags, sock, local_addr, server_hostname, ssl_handshake_timeout, ssl_shutdown_timeout, happy_eyeballs_delay, interleave)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1069\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_sock(\n\u001b[1;32m   1070\u001b[0m         exceptions, addrinfo, laddr_infos)\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/base_events.py:973\u001b[0m, in \u001b[0;36mBaseEventLoop._connect_sock\u001b[0;34m(self, exceptions, addr_info, local_addr_infos)\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno matching local address with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfamily\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 973\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock_connect(sock, address)\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/selector_events.py:634\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop.sock_connect\u001b[0;34m(self, sock, address)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# Needed to break cycles when an exception occurs.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/selector_events.py:674\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop._sock_connect_cb\u001b[0;34m(self, fut, sock, address)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    673\u001b[0m         \u001b[38;5;66;03m# Jump to any except clause below.\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(err, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnect call failed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maddress\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mBlockingIOError\u001b[39;00m, \u001b[38;5;167;01mInterruptedError\u001b[39;00m):\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# socket is still registered, the callback will be retried later\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connect call failed ('65.108.140.148', 443)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mClientConnectorError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     31\u001b[0m     tables_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/Dataset/Dataset/Round1_T2D/tables/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 32\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m (main(tables_path))\n",
      "Cell \u001b[0;32mIn[32], line 26\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(tables_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m table_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tables_path, table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(tables_path) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(pattern, table)]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_file \u001b[38;5;129;01min\u001b[39;00m tqdm(table_files):\n\u001b[0;32m---> 26\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_table(column_analysis, table_file, train_df, columns)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_df\n",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m, in \u001b[0;36mprocess_table\u001b[0;34m(column_analysis, table_path, train_df, columns)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_table\u001b[39m(column_analysis, table_path, train_df, columns):\n\u001b[1;32m      2\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(table_path)\n\u001b[0;32m----> 3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m column_analysis\u001b[38;5;241m.\u001b[39mclassify_columns_async(df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m      6\u001b[0m         row \u001b[38;5;241m=\u001b[39m {col: [entry\u001b[38;5;241m.\u001b[39mget(col, \u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns}\n",
      "Cell \u001b[0;32mIn[10], line 246\u001b[0m, in \u001b[0;36mColumnAnalysis.classify_columns_async\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:                \n\u001b[1;32m    245\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_entity(session, cell) \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m col_data \u001b[38;5;28;01mif\u001b[39;00m cell \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m count_cell \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m--> 246\u001b[0m     responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cell, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(col_data, responses):\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m#print(f\"{cell}-->{data}\")\u001b[39;00m\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNERtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[10], line 159\u001b[0m, in \u001b[0;36mColumnAnalysis.fetch_entity\u001b[0;34m(self, session, cell)\u001b[0m\n\u001b[1;32m    148\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://lamapi.hel.sintef.cloud/lookup/entity-retrieval\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    149\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: cell,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlamapi_demo_2023\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     ]\n\u001b[1;32m    158\u001b[0m }\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client.py:1167\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client.py:562\u001b[0m, in \u001b[0;36mClientSession._request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ceil_timeout(real_timeout\u001b[38;5;241m.\u001b[39mconnect):\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m         conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[1;32m    563\u001b[0m             req, traces\u001b[38;5;241m=\u001b[39mtraces, timeout\u001b[38;5;241m=\u001b[39mreal_timeout\n\u001b[1;32m    564\u001b[0m         )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerTimeoutError(\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection timeout \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto host \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url)\n\u001b[1;32m    568\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:540\u001b[0m, in \u001b[0;36mBaseConnector.connect\u001b[0;34m(self, req, traces, timeout)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m trace\u001b[38;5;241m.\u001b[39msend_connection_create_start()\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection(req, traces, timeout)\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m    542\u001b[0m         proto\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:901\u001b[0m, in \u001b[0;36mTCPConnector._create_connection\u001b[0;34m(self, req, traces, timeout)\u001b[0m\n\u001b[1;32m    899\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_proxy_connection(req, traces, timeout)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_direct_connection(req, traces, timeout)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proto\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:1209\u001b[0m, in \u001b[0;36mTCPConnector._create_direct_connection\u001b[0;34m(self, req, traces, timeout, client_error)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m last_exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m last_exc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:1178\u001b[0m, in \u001b[0;36mTCPConnector._create_direct_connection\u001b[0;34m(self, req, traces, timeout, client_error)\u001b[0m\n\u001b[1;32m   1175\u001b[0m port \u001b[38;5;241m=\u001b[39m hinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     transp, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_create_connection(\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_factory,\n\u001b[1;32m   1180\u001b[0m         host,\n\u001b[1;32m   1181\u001b[0m         port,\n\u001b[1;32m   1182\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1183\u001b[0m         ssl\u001b[38;5;241m=\u001b[39msslcontext,\n\u001b[1;32m   1184\u001b[0m         family\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1185\u001b[0m         proto\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproto\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1186\u001b[0m         flags\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1187\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhostname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m sslcontext \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1188\u001b[0m         local_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_addr,\n\u001b[1;32m   1189\u001b[0m         req\u001b[38;5;241m=\u001b[39mreq,\n\u001b[1;32m   1190\u001b[0m         client_error\u001b[38;5;241m=\u001b[39mclient_error,\n\u001b[1;32m   1191\u001b[0m     )\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientConnectorError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1193\u001b[0m     last_exc \u001b[38;5;241m=\u001b[39m exc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/connector.py:988\u001b[0m, in \u001b[0;36mTCPConnector._wrap_create_connection\u001b[0;34m(self, req, timeout, client_error, *args, **kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 988\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m client_error(req\u001b[38;5;241m.\u001b[39mconnection_key, exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mClientConnectorError\u001b[0m: Cannot connect to host lamapi.hel.sintef.cloud:443 ssl:default [Connect call failed ('65.108.140.148', 443)]"
     ]
    }
   ],
   "source": [
    "async def process_table(column_analysis, table_path, train_df, columns):\n",
    "    df = pd.read_csv(table_path)\n",
    "    result = await column_analysis.classify_columns_async(df.iloc[1:10])\n",
    "\n",
    "    for entry in result:\n",
    "        row = {col: [entry.get(col, None)] for col in columns}\n",
    "        train_df = pd.concat([train_df, pd.DataFrame(row)], ignore_index=True)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "async def main(tables_path):\n",
    "    column_analysis = ColumnAnalysis()\n",
    "    columns = [\n",
    "        'column_name', 'column_type', 'min_value', 'max_value', 'mean_value', 'std_dev', 'unique_count', 'special_values',\n",
    "        'average_length', 'min_length', 'max_length', 'all_caps', 'capitalized', 'hyphens', 'periods', 'commas', 'common_prefixes', 'common_suffixes',\n",
    "        'alphabetic_chars', 'digit_chars', 'special_chars', 'min_date', 'max_date', 'date_range', 'year_counts', 'month_counts'\n",
    "    ]\n",
    "    train_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    pattern = r'^\\.'\n",
    "\n",
    "    # Create a list of file paths, excluding files that start with a dot\n",
    "    table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path) if not re.match(pattern, table)]\n",
    "\n",
    "    for table_file in tqdm(table_files):\n",
    "        train_df = await process_table(column_analysis, table_file, train_df, columns)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables_path = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "    train_df = await (main(tables_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0e67a-29e7-4f5e-9774-21d2c0a4c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./R1_train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82ea33-cf51-4519-b56a-e78193196967",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['column_type','average_length', 'min_length', 'max_length', 'all_caps', 'capitalized', 'hyphens',\n",
    "    'periods', 'commas']][train_df['column_type'].isin(['NE_PERS', 'NE_LOC', 'NE_ORG', 'NE_OTHERS'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfe186-559d-4a4b-a568-eacafc41f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)\n",
    "\n",
    "R1_cea = [item[0]for item in R1_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866bea9-b680-4c47-b179-93f270b6bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Place\",\n",
    "    \"PopulatedPlace\",\n",
    "    \"City\",\n",
    "    \"Country\",\n",
    "    \"Region\",\n",
    "    \"Mountain\",\n",
    "    \"Island\",\n",
    "    \"Lake\",\n",
    "    \"River\",\n",
    "    \"Park\",\n",
    "    \"Building\",\n",
    "    \"HistoricPlace\",\n",
    "    \"Monument\",\n",
    "    \"Bridge\",\n",
    "    \"Road\",\n",
    "    \"Airport\",\n",
    "    \"Person\",\n",
    "    \"Artist\",\n",
    "    \"Athlete\",\n",
    "    \"Politician\",\n",
    "    \"Scientist\",\n",
    "    \"Writer\",\n",
    "    \"Actor\",\n",
    "    \"Musician\",\n",
    "    \"MilitaryPerson\",\n",
    "    \"Religious\",\n",
    "    \"Royalty\",\n",
    "    \"Criminal\",\n",
    "    \"Organisation\",\n",
    "    \"Company\",\n",
    "    \"EducationalInstitution\",\n",
    "    \"PoliticalParty\",\n",
    "    \"SportsTeam\",\n",
    "    \"Non-ProfitOrganisation\",\n",
    "    \"GovernmentAgency\",\n",
    "    \"ReligiousOrganisation\",\n",
    "    \"Band\",\n",
    "    \"Library\",\n",
    "    \"Museum\",\n",
    "    \"Hospital\",\n",
    "    \"University\",\n",
    "    \"TradeUnion\"\n",
    "]\n",
    "\n",
    "# Mapping of subtypes to macro classes\n",
    "mapping = {\n",
    "    \"Place\": [\"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\", \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"],\n",
    "    \"Person\": [\"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\", \"Religious\", \"Royalty\", \"Criminal\"],\n",
    "    \"Organisation\": [\"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\", \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\"],\n",
    "    \"Institution\": [\"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab481172-9273-43e5-b4d2-d1011cec6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R1_cea).any()\n",
    "        if NE_flag:\n",
    "            joined_cells = column.str.cat(sep='-')        \n",
    "            doc = nlp(joined_cells)\n",
    "            entities = {\"ORG\": [], \"PERS\": [], \"LOC\": [], \"OTHERS\": []}\n",
    "\n",
    "            # Extract entities and classify them\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"ORG\":\n",
    "                    entities[\"ORG\"].append(ent.text)\n",
    "                elif ent.label_ == \"PERSON\":\n",
    "                    entities[\"PERS\"].append(ent.text)\n",
    "                elif ent.label_ == \"GPE\" or ent.label_ == \"FAC\":  # GPE (Geopolitical Entity)\n",
    "                    entities[\"LOC\"].append(ent.text)\n",
    "                else:\n",
    "                    entities[\"OTHERS\"].append(ent.text)\n",
    "            \n",
    "            # Find the key of the longest entities list\n",
    "            longest_key = max(entities, key=lambda k: len(entities[k]))\n",
    "            print(f\"{joined_cells} --> The key of the longest list is '{longest_key}'\")\n",
    "                    \n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da63443-b6a5-4060-b097-e6e4a2fb9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)\n",
    "\n",
    "R3_cea = [item[0]for item in R3_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c261e-3ff1-41ca-8cde-34c090d858e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R3_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c0a1c-045b-4cc6-ac29-d60215b8fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_2T_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf378a-2f88-44c0-a551-843d7dc9bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_2T_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208edf55-99eb-4018-a0a7-031dbd192a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b461089-7f54-4ed5-9aef-669855493e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cad483-e185-4211-9b4b-09f2e32e504a",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7a08be3d-4eea-4790-9169-2be28761cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 and df2 are the same\n",
    "\n",
    "df1 = pd.read_csv(\"./R1_train_df.csv\")\n",
    "#df2 = pd.read_csv(\"./R3_train_df.csv\")\n",
    "df3 = pd.read_csv(\"./R4_train_df.csv\")\n",
    "df4 = pd.read_csv(\"./HT2_train_df.csv\")\n",
    "\n",
    "# filtering because otherwise the model gets values too high\n",
    "df3 = df3[(df3['max_value'] <= 1.000000e+10) ]\n",
    "\n",
    "result = pd.concat([df3, df4, df1], axis=0)\n",
    "result.drop(['date_range', 'year_counts', 'month_counts'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9034c5ae-58e7-4948-8e21-35a9472cde1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>column_type</th>\n",
       "      <th>min_value</th>\n",
       "      <th>max_value</th>\n",
       "      <th>mean_value</th>\n",
       "      <th>std_dev</th>\n",
       "      <th>unique_count</th>\n",
       "      <th>special_values</th>\n",
       "      <th>average_length</th>\n",
       "      <th>min_length</th>\n",
       "      <th>...</th>\n",
       "      <th>common_prefixes</th>\n",
       "      <th>common_suffixes</th>\n",
       "      <th>alphabetic_chars</th>\n",
       "      <th>digit_chars</th>\n",
       "      <th>special_chars</th>\n",
       "      <th>min_date</th>\n",
       "      <th>max_date</th>\n",
       "      <th>valid_urls</th>\n",
       "      <th>address_count</th>\n",
       "      <th>valid_emails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>col1</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>80521.0000</td>\n",
       "      <td>5.186950e+05</td>\n",
       "      <td>310598.222222</td>\n",
       "      <td>149912.648851</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>col2</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>101.0000</td>\n",
       "      <td>1.426000e+03</td>\n",
       "      <td>500.666667</td>\n",
       "      <td>435.768797</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>col1</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>85.0000</td>\n",
       "      <td>3.810000e+02</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>100.807848</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>col2</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>3.2700</td>\n",
       "      <td>1.254100e+01</td>\n",
       "      <td>6.648000</td>\n",
       "      <td>2.582835</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>col1</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>17.3662</td>\n",
       "      <td>5.411520e+01</td>\n",
       "      <td>33.123111</td>\n",
       "      <td>12.269156</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Met</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>890.0000</td>\n",
       "      <td>9.640000e+02</td>\n",
       "      <td>914.333333</td>\n",
       "      <td>25.944385</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Grid Ref</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Area</td>\n",
       "      <td>STRING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>?</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>2.309401</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Trips</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.234567e+06</td>\n",
       "      <td>154799.375000</td>\n",
       "      <td>408114.182688</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11954 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    column_name column_type   min_value     max_value     mean_value  \\\n",
       "1          col1      NUMBER  80521.0000  5.186950e+05  310598.222222   \n",
       "2          col2      NUMBER    101.0000  1.426000e+03     500.666667   \n",
       "10         col1      NUMBER     85.0000  3.810000e+02     205.000000   \n",
       "11         col2      NUMBER      3.2700  1.254100e+01       6.648000   \n",
       "21         col1      NUMBER     17.3662  5.411520e+01      33.123111   \n",
       "..          ...         ...         ...           ...            ...   \n",
       "318         Met      NUMBER    890.0000  9.640000e+02     914.333333   \n",
       "319    Grid Ref      NUMBER         NaN           NaN            NaN   \n",
       "320        Area      STRING         NaN           NaN            NaN   \n",
       "321           ?      NUMBER      2.0000  9.000000e+00       5.333333   \n",
       "322       Trips      NUMBER      1.0000  1.234567e+06  154799.375000   \n",
       "\n",
       "           std_dev  unique_count  special_values  average_length  min_length  \\\n",
       "1    149912.648851           9.0             NaN             NaN         NaN   \n",
       "2       435.768797           9.0             NaN             NaN         NaN   \n",
       "10      100.807848           9.0             NaN             NaN         NaN   \n",
       "11        2.582835           9.0             NaN             NaN         NaN   \n",
       "21       12.269156           9.0             NaN             NaN         NaN   \n",
       "..             ...           ...             ...             ...         ...   \n",
       "318      25.944385           9.0             NaN             NaN         NaN   \n",
       "319            NaN           9.0             NaN             NaN         NaN   \n",
       "320            NaN           NaN             NaN             7.0         7.0   \n",
       "321       2.309401           6.0             NaN             NaN         NaN   \n",
       "322  408114.182688           5.0             NaN             NaN         NaN   \n",
       "\n",
       "     ...  common_prefixes  common_suffixes  alphabetic_chars  digit_chars  \\\n",
       "1    ...              NaN              NaN               NaN          NaN   \n",
       "2    ...              NaN              NaN               NaN          NaN   \n",
       "10   ...              NaN              NaN               NaN          NaN   \n",
       "11   ...              NaN              NaN               NaN          NaN   \n",
       "21   ...              NaN              NaN               NaN          NaN   \n",
       "..   ...              ...              ...               ...          ...   \n",
       "318  ...              NaN              NaN               NaN          NaN   \n",
       "319  ...              NaN              NaN               NaN          NaN   \n",
       "320  ...              NaN              NaN              54.0          0.0   \n",
       "321  ...              NaN              NaN               NaN          NaN   \n",
       "322  ...              NaN              NaN               NaN          NaN   \n",
       "\n",
       "     special_chars  min_date  max_date  valid_urls  address_count  \\\n",
       "1              NaN       NaN       NaN         NaN            NaN   \n",
       "2              NaN       NaN       NaN         NaN            NaN   \n",
       "10             NaN       NaN       NaN         NaN            NaN   \n",
       "11             NaN       NaN       NaN         NaN            NaN   \n",
       "21             NaN       NaN       NaN         NaN            NaN   \n",
       "..             ...       ...       ...         ...            ...   \n",
       "318            NaN       NaN       NaN         NaN            NaN   \n",
       "319            NaN       NaN       NaN         NaN            NaN   \n",
       "320            9.0       NaN       NaN         NaN            NaN   \n",
       "321            NaN       NaN       NaN         NaN            NaN   \n",
       "322            NaN       NaN       NaN         NaN            NaN   \n",
       "\n",
       "     valid_emails  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "10            NaN  \n",
       "11            NaN  \n",
       "21            NaN  \n",
       "..            ...  \n",
       "318           NaN  \n",
       "319           NaN  \n",
       "320           NaN  \n",
       "321           NaN  \n",
       "322           NaN  \n",
       "\n",
       "[11954 rows x 26 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dae4f8a6-bcf8-496b-b46c-2fc3aeaebc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "formats = ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "for fmt in formats:\n",
    "    result['min_date'] = pd.to_datetime(result['min_date'], format=fmt, errors='coerce')\n",
    "    result['max_date'] = pd.to_datetime(result['max_date'], format=fmt, errors='coerce')\n",
    "\n",
    "\n",
    "result['min_month'] = result['min_date'].dt.month\n",
    "result['min_year'] = result['min_date'].dt.year\n",
    "result['max_month'] = result['max_date'].dt.month\n",
    "result['max_year'] = result['max_date'].dt.year\n",
    "result = result.dropna(subset=['column_type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "85e8ad06-1ed2-4f47-875c-657f2c33a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.iloc[:, 2:26] = result.iloc[:, 2:26].fillna(-1) \n",
    "result.iloc[:, 26:30] = result.iloc[:, 26:30].fillna(0)  # fill the ['min_month', 'min_year', 'max_month', 'max_year'] \n",
    "\n",
    "X = result.drop(['max_date', 'min_date', 'column_name', 'column_type'], axis=1)  # Drop the target column from features\n",
    "y = label_encoder.fit_transform(result['column_type'].values)\n",
    "\n",
    "# One-hot encode the target variable for multiclass classification\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a6812f0b-1932-49f4-a966-8ea964af492b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADDRESS</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DATETIME</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NE_LOC</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NE_ORG</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NE_OTHERS</td>\n",
       "      <td>1650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NE_PERS</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NOA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NUMBER</td>\n",
       "      <td>8660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>STRING</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_type  count\n",
       "0     ADDRESS      8\n",
       "1    DATETIME    229\n",
       "2      NE_LOC    744\n",
       "3      NE_ORG    269\n",
       "4   NE_OTHERS   1650\n",
       "5     NE_PERS    130\n",
       "6         NOA      1\n",
       "7      NUMBER   8660\n",
       "8      STRING    257"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby('column_type').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "31162f44-ef54-4019-9e7d-e5f60d39d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42, k_neighbors=2)\n",
    "#X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e2a2f818-71cb-4c7c-b46e-1d367c0a20a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5200 - loss: 291905.2500 - val_accuracy: 0.8891 - val_loss: 0.4713\n",
      "Epoch 2/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7462 - loss: 2142891.0000 - val_accuracy: 0.8996 - val_loss: 0.4191\n",
      "Epoch 3/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8026 - loss: 491.2078 - val_accuracy: 0.9048 - val_loss: 0.3792\n",
      "Epoch 4/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8339 - loss: 636576.3125 - val_accuracy: 0.9059 - val_loss: 0.3554\n",
      "Epoch 5/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8525 - loss: 21.6121 - val_accuracy: 0.9064 - val_loss: 0.3343\n",
      "Epoch 6/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8589 - loss: 5.4494 - val_accuracy: 0.9053 - val_loss: 0.3233\n",
      "Epoch 7/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8659 - loss: 410.4388 - val_accuracy: 0.9048 - val_loss: 0.3149\n",
      "Epoch 8/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8695 - loss: 1.6975 - val_accuracy: 0.9027 - val_loss: 0.3180\n",
      "Epoch 9/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8704 - loss: 6.5591 - val_accuracy: 0.9022 - val_loss: 0.3131\n",
      "Epoch 10/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8684 - loss: 1.2201 - val_accuracy: 0.8860 - val_loss: 0.3357\n",
      "Epoch 11/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8733 - loss: 4.8108 - val_accuracy: 0.9048 - val_loss: 0.3025\n",
      "Epoch 12/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8752 - loss: 8.7476 - val_accuracy: 0.9048 - val_loss: 0.2964\n",
      "Epoch 13/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8676 - loss: 2.6789 - val_accuracy: 0.9053 - val_loss: 0.2940\n",
      "Epoch 14/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8817 - loss: 103.4722 - val_accuracy: 0.9038 - val_loss: 0.2999\n",
      "Epoch 15/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8659 - loss: 40.7951 - val_accuracy: 0.9043 - val_loss: 0.2973\n",
      "Epoch 16/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8821 - loss: 1.1683 - val_accuracy: 0.9027 - val_loss: 0.3022\n",
      "Epoch 17/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8756 - loss: 1.3940 - val_accuracy: 0.9027 - val_loss: 0.2964\n",
      "Epoch 18/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8778 - loss: 0.7979 - val_accuracy: 0.9048 - val_loss: 0.2945\n",
      "Epoch 19/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8747 - loss: 0.4521 - val_accuracy: 0.9027 - val_loss: 0.2931\n",
      "Epoch 20/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8751 - loss: 1.1894 - val_accuracy: 0.9043 - val_loss: 0.2912\n",
      "Epoch 21/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8753 - loss: 5.9538 - val_accuracy: 0.8991 - val_loss: 0.2924\n",
      "Epoch 22/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8818 - loss: 0.5761 - val_accuracy: 0.9053 - val_loss: 0.2964\n",
      "Epoch 23/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8753 - loss: 15.1100 - val_accuracy: 0.9043 - val_loss: 0.2876\n",
      "Epoch 24/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8823 - loss: 1.1803 - val_accuracy: 0.9069 - val_loss: 0.2872\n",
      "Epoch 25/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8783 - loss: 0.5227 - val_accuracy: 0.9069 - val_loss: 0.2838\n",
      "Epoch 26/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8810 - loss: 0.4041 - val_accuracy: 0.9069 - val_loss: 0.2788\n",
      "Epoch 27/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8866 - loss: 0.4714 - val_accuracy: 0.9079 - val_loss: 0.2757\n",
      "Epoch 28/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8823 - loss: 0.7253 - val_accuracy: 0.9085 - val_loss: 0.2718\n",
      "Epoch 29/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8822 - loss: 1.0151 - val_accuracy: 0.9079 - val_loss: 0.2699\n",
      "Epoch 30/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8800 - loss: 1.0807 - val_accuracy: 0.9079 - val_loss: 0.2708\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8878 - loss: 0.2920\n",
      "Test Accuracy: 89.04%\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3626d3a5-4a1e-44b0-bb6d-b72421c5f76a",
   "metadata": {},
   "source": [
    "## New predictions (should be correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b8cdab6c-5a61-46e8-b645-ec3f58b29a80",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'NERtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m'\u001b[39m: people})\n\u001b[1;32m     38\u001b[0m column_analysis \u001b[38;5;241m=\u001b[39m ColumnAnalysis()\n\u001b[0;32m---> 39\u001b[0m df_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m column_analysis\u001b[38;5;241m.\u001b[39mclassify_columns_async(df)\n\u001b[1;32m     41\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_dev\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_values\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_caps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapitalized\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyphens\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiods\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommon_prefixes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommon_suffixes\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malphabetic_chars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdigit_chars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_chars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_urls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_emails\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_date\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_counts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth_counts\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     46\u001b[0m ]\n\u001b[1;32m     48\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "Cell \u001b[0;32mIn[166], line 248\u001b[0m, in \u001b[0;36mColumnAnalysis.classify_columns_async\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    244\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cell, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(col_data, responses):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m#print(f\"{cell}-->{data}\")\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNERtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m combine_scores(data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjaccard_score\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124med_score\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m:\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNE_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNERtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'NERtype'"
     ]
    }
   ],
   "source": [
    "datetime_strings = [\n",
    "    \"2023-02-15 08:30:00\",\n",
    "    \"2023-03-20 18:45:00\",\n",
    "    \"2023-04-10 10:00:00\",\n",
    "    \"2023-05-05 14:20:00\"\n",
    "]\n",
    "\n",
    "people = [\n",
    "    \"John Smith\",\n",
    "    \"Mary Johnson\",\n",
    "    \"James Williams\",\n",
    "    \"Patricia Brown\",\n",
    "    \"Michael Davis\",\n",
    "    \"Jennifer Miller\",\n",
    "    \"William Wilson\",\n",
    "    \"Linda Moore\",\n",
    "    \"David Taylor\",\n",
    "    \"Barbara Anderson\"\n",
    "]\n",
    "\n",
    "cities = [\n",
    "    \"Tokyo lake\",\n",
    "    \"New York City lake\",\n",
    "    \"Paris lake\",\n",
    "    \"London lake\",\n",
    "    \"Dubai lake\",\n",
    "    \"Singapore\",\n",
    "    \"Sydney\",\n",
    "    \"Berlin\",\n",
    "    \"Hong Kong\",\n",
    "    \"Rio de Janeiro\"\n",
    "]\n",
    "\n",
    "\n",
    "#df = pd.DataFrame({'desc': [\"minore di 3 anni\" for i in range(0, 10)]})\n",
    "#df = pd.DataFrame({'desc': datetime_strings})\n",
    "df = pd.DataFrame({'desc': people})\n",
    "column_analysis = ColumnAnalysis()\n",
    "df_feat = await column_analysis.classify_columns_async(df)\n",
    "\n",
    "columns = [\n",
    "    'min_value', 'max_value', 'mean_value', 'std_dev', 'unique_count', 'special_values',\n",
    "    'average_length', 'min_length', 'max_length', 'all_caps', 'capitalized', 'hyphens', 'periods', 'commas', 'common_prefixes', 'common_suffixes',\n",
    "    'alphabetic_chars', 'digit_chars', 'special_chars', 'valid_urls', 'address_count', 'valid_emails', 'min_date',\n",
    "    'max_date', 'date_range', 'year_counts', 'month_counts'\n",
    "]\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "for entry in df_feat:\n",
    "    row = {col: [entry.get(col, None)] for col in columns}\n",
    "    test_df = pd.DataFrame(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f61aa1-b18c-434a-8397-1ead3f1dd74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a563b-19b2-498c-8b7b-3515d88f890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'test_df' is your DataFrame name\n",
    "test_df['min_date'] = pd.to_datetime(test_df['min_date'], format='%Y-%m-%d %H:%M:%S')\n",
    "test_df['max_date'] = pd.to_datetime(test_df['max_date'], format='%Y-%m-%d %H:%M:%S')\n",
    "test_df['min_month'] = test_df['min_date'].dt.month\n",
    "test_df['min_year'] = test_df['min_date'].dt.year\n",
    "test_df['max_month'] = test_df['max_date'].dt.month\n",
    "test_df['max_year'] = test_df['max_date'].dt.year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77de4d-2710-4a9e-9679-205013731527",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(['max_date', 'min_date', 'date_range', 'year_counts', 'month_counts'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9fc58-183b-4e3e-9ef6-45b92258fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in columns 2 to 20 with -1\n",
    "test_df.iloc[:, 0:22] = test_df.iloc[:, 0:22].fillna(-1.0)\n",
    "\n",
    "# Fill NaN values in columns 23 to 26 with 0\n",
    "test_df.iloc[:, 22:27] = test_df.iloc[:, 22:27].fillna(0.0)\n",
    "\n",
    "test_df = test_df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e2d23-3642-4717-857e-11211887d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for el in test_df.columns:\n",
    "#    print(f\"{el} --> {test_df.at[0,el]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43349d-a930-4bc3-af64-ba11523853a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_df)\n",
    "\n",
    "# Convert predictions to class labels if necessary\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "\n",
    "# Display the predictions\n",
    "\n",
    "print((predicted_classes))\n",
    "print(label_encoder.inverse_transform(predicted_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

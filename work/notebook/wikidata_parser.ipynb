{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497013ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SPARQLWrapper in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: rdflib>=6.1.1 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from SPARQLWrapper) (7.0.0)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.6.1)\n",
      "Requirement already satisfied: six in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c242f0f9-c9e3-4f3b-8d15-96050eeeb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo import *\n",
    "from pymongo import errors\n",
    "import configparser\n",
    "from json.decoder import JSONDecodeError\n",
    "from collections import Counter\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import time\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef38472-3087-45f2-ae9b-fdd8757076a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['mongo:27017'], document_class=dict, tz_aware=False, connect=True)\n"
     ]
    }
   ],
   "source": [
    "# MongoDB connection setup\n",
    "MONGO_ENDPOINT, MONGO_ENDPOINT_PORT = os.environ[\"MONGO_ENDPOINT\"].split(\":\")\n",
    "MONGO_ENDPOINT_PORT = int(MONGO_ENDPOINT_PORT)\n",
    "MONGO_ENDPOINT_USERNAME = os.environ[\"MONGO_INITDB_ROOT_USERNAME\"]\n",
    "MONGO_ENDPOINT_PASSWORD = os.environ[\"MONGO_INITDB_ROOT_PASSWORD\"]\n",
    "DB_NAME = f\"wikidata\"\n",
    "\n",
    "client = MongoClient(MONGO_ENDPOINT, MONGO_ENDPOINT_PORT, username=MONGO_ENDPOINT_USERNAME, password=MONGO_ENDPOINT_PASSWORD)\n",
    "print(client)\n",
    "\n",
    "log_c = client.wikidata.log\n",
    "items_c = client[DB_NAME].items\n",
    "objects_c = client[DB_NAME].objects\n",
    "literals_c = client[DB_NAME].literals\n",
    "types_c = client[DB_NAME].types\n",
    "\n",
    "c_ref = {\n",
    "    \"items\": items_c,\n",
    "    \"objects\":objects_c, \n",
    "    \"literals\":literals_c, \n",
    "    \"types\":types_c\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16daa576-49bf-4fd5-960f-a034684916a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_buffer(buffer):\n",
    "    for key in buffer:\n",
    "        if len(buffer[key]) > 0:\n",
    "            c_ref[key].insert_many(buffer[key])\n",
    "            buffer[key] = []\n",
    "\n",
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ccbd4c-be3e-4b23-a8b8-d93e3f9397fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/def_mapping.json\"\n",
    "\n",
    "try:\n",
    "    # Open the JSON file for reading\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        mapping = json.load(json_file)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{json_file_path}' not found.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON data: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b855d5-e433-489e-a4fc-136eaba530e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_processed = 0\n",
    "num_entities_processed = 0\n",
    "\n",
    "def update_average_size(new_size):\n",
    "    global total_size_processed, num_entities_processed\n",
    "    total_size_processed += new_size\n",
    "    num_entities_processed += 1\n",
    "    return total_size_processed / num_entities_processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f102de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_superclasses(entity_id):\n",
    "    \"\"\"\n",
    "    Retrieve all superclasses of a given Wikidata entity ID.\n",
    "\n",
    "    Args:\n",
    "        entity_id (str): The ID of the entity (e.g., \"Q207784\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are superclass IDs, and values are their labels.\n",
    "    \"\"\"\n",
    "    # Define the SPARQL endpoint and query\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?superclass ?superclassLabel WHERE {{\n",
    "      wd:{entity_id} (wdt:P279)* ?superclass.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to query the SPARQL endpoint with retries\n",
    "    def query_wikidata(sparql_client, query, retries=3, delay=5):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                sparql_client.setQuery(query)\n",
    "                sparql_client.setReturnFormat(JSON)\n",
    "                results = sparql_client.query().convert()\n",
    "                return results\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):  # Handle Too Many Requests error\n",
    "                    print(f\"Rate limit hit. Retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    break\n",
    "        return None\n",
    "\n",
    "    # Set up the SPARQL client\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "\n",
    "    # Execute the query with retries\n",
    "    results = query_wikidata(sparql, query)\n",
    "\n",
    "    # Process results and return as a dictionary\n",
    "    if results:\n",
    "        superclass_dict = {}\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            superclass_id = result[\"superclass\"][\"value\"].split(\"/\")[-1]  # Extract entity ID from the URI\n",
    "            label = result[\"superclassLabel\"][\"value\"]\n",
    "            superclass_dict[label] = int(superclass_id[1:])\n",
    "        return list(superclass_dict.values())\n",
    "    else:\n",
    "        print(\"Failed to retrieve data after multiple attempts.\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b8114a-6765-4c4c-ab18-1a5c17c8aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/375297 [00:59<1040:33:29,  9.98s/it]\n",
      "  0%|          | 1/332586 [00:00<20:05:48,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belgium - Q31\n",
      "--> LOC \n",
      "--> ORG \n",
      "[Q3624078] - # superclasses: 54\n",
      "[Q43702] - # superclasses: 48\n",
      "[Q6256] - # superclasses: 44\n",
      "[Q20181813] - # superclasses: 55\n",
      "[Q185441] - # superclasses: 55\n",
      "[Q1250464] - # superclasses: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/347891 [00:07<407:56:49,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q113489728] - # superclasses: 55\n",
      "Superclasses: [65] {61961344, 1048835, 1140229, 21157127, 618123, 1646605, 3624078, 178706, 4835091, 27096213, 24229398, 211606, 58778, 123349660, 1896989, 1250464, 6671777, 3778211, 3455524, 874405, 26713767, 96196009, 27096235, 124711467, 15642541, 484652, 35120, 20181813, 43702, 23956024, 854457, 58415929, 106559804, 124711484, 131085629, 488383, 18810687, 113489728, 106668099, 155076, 1063239, 8191099, 28108, 53617489, 1639378, 2221906, 16887380, 16334295, 16334298, 43229, 99527517, 25404640, 185441, 177634, 98119401, 82794, 7275, 6256, 16686448, 103940464, 16562419, 7210356, 22676603, 56061, 53617407}\n",
      "_______________________\n",
      "happiness - Q8\n",
      "--> OTHERS \n",
      "[Q331769] - # superclasses: 28\n",
      "[Q60539479] - # superclasses: 18\n",
      "Superclasses: [32] {3968640, 67518978, 282250, 937228, 3249551, 7048977, 1322005, 12047512, 12047513, 111752858, 1190554, 1293220, 5127848, 2996394, 30241068, 483247, 35120, 12145458, 3505845, 58415929, 488383, 17320256, 9415, 20937557, 60539479, 813912, 99527517, 41537118, 7268708, 781413, 64732777, 331769}\n",
      "_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/344631 [00:08<283:17:22,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Washington - Q23\n",
      "--> PERS \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/363202 [00:09<213:58:22,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q5] - # superclasses: 25\n",
      "Superclasses: [25] {66394244, 5, 24229398, 164509, 3778211, 795052, 12898224, 35120, 106559804, 72638, 488383, 223557, 7239, 154954, 215627, 110551885, 53617489, 4406616, 729, 26401003, 27043950, 10855152, 159344, 103940464, 53617407}\n",
      "_______________________\n",
      "Jack Bauer - Q24\n",
      "--> OTHERS \n",
      "[Q15632617] - # superclasses: 21\n",
      "[Q15773317] - # superclasses: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/362502 [00:12<252:39:32,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q20085850] - # superclasses: 12\n",
      "Superclasses: [23] {122192387, 15773317, 14897293, 7048977, 24229398, 28020127, 115537581, 35120, 27277631, 488383, 97498056, 3542731, 2593744, 53617489, 96789464, 20085850, 95074, 30017383, 15632617, 16686448, 103940464, 64728693, 115257598}\n",
      "_______________________\n",
      "Douglas Adams - Q42\n",
      "--> PERS \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/374430 [00:13<191:36:45,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q5] - # superclasses: 25\n",
      "Superclasses: [25] {66394244, 5, 24229398, 164509, 3778211, 795052, 12898224, 35120, 106559804, 72638, 488383, 223557, 7239, 154954, 215627, 110551885, 53617489, 4406616, 729, 26401003, 27043950, 10855152, 159344, 103940464, 53617407}\n",
      "_______________________\n",
      "Paul Otlet - Q1868\n",
      "--> PERS \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/376125 [00:13<144:04:17,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q5] - # superclasses: 25\n",
      "Superclasses: [25] {66394244, 5, 24229398, 164509, 3778211, 795052, 12898224, 35120, 106559804, 72638, 488383, 223557, 7239, 154954, 215627, 110551885, 53617489, 4406616, 729, 26401003, 27043950, 10855152, 159344, 103940464, 53617407}\n",
      "_______________________\n",
      "Wikidata - Q2013\n",
      "--> OTHERS \n",
      "[Q33120876] - # superclasses: 67\n",
      "[Q638153] - # superclasses: 52\n",
      "[Q36509592] - # superclasses: 54\n",
      "[Q15633582] - # superclasses: 52\n",
      "[Q593744] - # superclasses: 40\n",
      "[Q7094076] - # superclasses: 49\n",
      "[Q33002955] - # superclasses: 44\n",
      "[Q114955954] - # superclasses: 15\n",
      "[Q115471117] - # superclasses: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/358917 [00:25<475:11:11,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1293664] - # superclasses: 5\n",
      "Superclasses: [149] {107435521, 67518978, 121773569, 5276676, 131093, 110854677, 216601, 4393498, 119648796, 3523102, 382497, 15140858, 3427877, 17537576, 2668072, 21550639, 55915575, 213051, 3924032, 45025858, 215111, 63344712, 797769, 4833865, 11348, 390741, 170584, 115096159, 33120867, 33120876, 273005, 217594, 105227892, 40056, 864377, 61961344, 123407493, 1347208, 110832782, 107307154, 31464082, 386724, 15621286, 5127848, 171, 180907, 15633582, 114955954, 49848, 189112, 12488383, 5371079, 340169, 6129866, 638153, 115636432, 115095765, 11491547, 54938846, 5155040, 262372, 7397, 782566, 1076968, 98119401, 3622126, 229102, 182003, 19833078, 24034552, 2198779, 1948412, 179976, 2623243, 1914636, 115471117, 937228, 7048977, 11028, 6423319, 14827288, 35825432, 115217689, 12774177, 5469988, 35120, 788790, 35127, 1554231, 58415929, 59138870, 203066, 1172284, 7094076, 748349, 11953984, 8513, 82753, 36161, 370502, 151885, 11826511, 593744, 53617489, 115668308, 20937557, 130901, 99527517, 1293664, 42848, 130342754, 16686448, 103940464, 17538423, 286583, 1063801, 451967, 1340800, 3249551, 246672, 104637332, 36509592, 58778, 37866906, 61788060, 732577, 1754533, 2424752, 629173, 54933429, 108637623, 488383, 1639361, 1209283, 1714118, 33002955, 1786828, 1639378, 16334295, 130862553, 16334298, 234460, 47461344, 37787110, 4303335, 1945067, 107223533, 121773562, 15916540}\n",
      "_______________________\n",
      "Portugal - Q45\n",
      "--> LOC \n",
      "[Q3624078] - # superclasses: 54\n",
      "[Q6256] - # superclasses: 44\n",
      "[Q20181813] - # superclasses: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/365753 [00:28<419:49:39,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q113489728] - # superclasses: 55\n",
      "Superclasses: [58] {61961344, 1048835, 21157127, 618123, 1646605, 3624078, 178706, 4835091, 27096213, 24229398, 211606, 58778, 123349660, 1896989, 6671777, 3778211, 3455524, 874405, 26713767, 96196009, 27096235, 124711467, 15642541, 35120, 20181813, 23956024, 854457, 58415929, 106559804, 124711484, 131085629, 488383, 18810687, 113489728, 106668099, 155076, 1063239, 53617489, 1639378, 2221906, 16887380, 16334295, 16334298, 43229, 99527517, 25404640, 177634, 98119401, 82794, 7275, 6256, 16686448, 103940464, 16562419, 7210356, 8191099, 56061, 53617407}\n",
      "_______________________\n",
      "Antarctica - Q51\n",
      "--> OTHERS \n",
      "--> LOC \n",
      "[Q5107] - # superclasses: 26\n",
      "[Q82794] - # superclasses: 15\n",
      "[Q312461] - # superclasses: 44\n",
      "[Q2418896] - # superclasses: 16\n",
      "Superclasses: [56] {61961344, 1048835, 1970309, 15989253, 618123, 312461, 178706, 4835091, 27096213, 211606, 24229398, 58778, 27096220, 123349660, 6671777, 3455524, 26713767, 115385384, 124711467, 27096235, 15642541, 35145263, 35120, 23956024, 58415929, 854457, 124711484, 106559804, 131085629, 488383, 18810687, 106668099, 223557, 16686022, 205895, 1063239, 20719696, 53617489, 2221906, 1639378, 2418896, 16334295, 4406616, 16334298, 43229, 99527517, 25404640, 98119401, 82794, 103940464, 16686448, 5107, 16562419, 7210356, 56061, 53617407}\n",
      "_______________________\n",
      "penis - Q58\n",
      "computer - Q68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/373728 [00:32<416:00:01,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet - Q75\n",
      "--> OTHERS \n",
      "[Q11224256] - # superclasses: 14\n",
      "[Q1301371] - # superclasses: 13\n",
      "[Q14208553] - # superclasses: 5\n",
      "Superclasses: [16] {11224256, 6671777, 577764, 386724, 1900326, 14208553, 1068715, 121359, 994895, 35120, 2424752, 16686448, 58778, 1301371, 99527517, 488383}\n",
      "_______________________\n",
      "pneumonoultramicroscopicsilicovolcanoconiosis - Q102\n",
      "Supercalifragilisticexpialidocious - Q103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/388350 [00:34<373:07:45,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> OTHERS \n",
      "[Q105543609] - # superclasses: 9\n",
      "Superclasses: [9] {386724, 838948, 15621286, 17537576, 16686448, 35120, 105543609, 2188189, 488383}\n",
      "_______________________\n",
      "November - Q125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/392054 [00:35<296:33:18,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion - Q140\n",
      "--> OTHERS \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 143>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m types_list:\n\u001b[0;32m    269\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m retrieve_superclasses(el)\n\u001b[1;32m--> 270\u001b[0m     superclasses \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_superclasses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with your entity ID\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] - # superclasses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(superclasses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuperclasses: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(total))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(total)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mretrieve_superclasses\u001b[1;34m(entity_id)\u001b[0m\n\u001b[0;32m     38\u001b[0m sparql \u001b[38;5;241m=\u001b[39m SPARQLWrapper(endpoint_url)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Execute the query with retries\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mquery_wikidata\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Process results and return as a dictionary\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results:\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mretrieve_superclasses.<locals>.query_wikidata\u001b[1;34m(sparql_client, query, retries, delay)\u001b[0m\n\u001b[0;32m     24\u001b[0m     sparql_client\u001b[38;5;241m.\u001b[39msetQuery(query)\n\u001b[0;32m     25\u001b[0m     sparql_client\u001b[38;5;241m.\u001b[39msetReturnFormat(JSON)\n\u001b[1;32m---> 26\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msparql_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert()\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py:960\u001b[0m, in \u001b[0;36mSPARQLWrapper.query\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryResult\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    943\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    Execute the query.\u001b[39;00m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;124;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py:926\u001b[0m, in \u001b[0;36mSPARQLWrapper._query\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    924\u001b[0m         response \u001b[38;5;241m=\u001b[39m urlopener(request, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 926\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnFormat\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\urllib\\request.py:1352\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m-> 1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1354\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wikidata_dump_path = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/latest-all.json.bz2'\n",
    "initial_estimated_average_size = 800\n",
    "BATCH_SIZE = 100 # Number of entities to insert in a single batch\n",
    "compressed_file_size = os.path.getsize(wikidata_dump_path)\n",
    "initial_total_lines_estimate = compressed_file_size / initial_estimated_average_size\n",
    "\n",
    "DATATYPES_MAPPINGS = {\n",
    "    'external-id': 'STRING',\n",
    "    'quantity': 'NUMBER',\n",
    "    'globe-coordinate': 'STRING',\n",
    "    'string': 'STRING',\n",
    "    'monolingualtext': 'STRING',\n",
    "    'commonsMedia': 'STRING',\n",
    "    'time': 'DATETIME',\n",
    "    'url': 'STRING',\n",
    "    'geo-shape': 'GEOSHAPE',\n",
    "    'math': 'MATH',\n",
    "    'musical-notation': 'MUSICAL_NOTATION',\n",
    "    'tabular-data': 'TABULAR_DATA'\n",
    "}\n",
    "DATATYPES = list(set(DATATYPES_MAPPINGS.values()))\n",
    "\n",
    "buffer = {\n",
    "    \"items\": [],\n",
    "    \"objects\": [], \n",
    "    \"literals\": [], \n",
    "    \"types\": []\n",
    "}\n",
    "\n",
    "def check_skip(obj, datatype):\n",
    "    temp = obj.get(\"mainsnak\", obj)\n",
    "    if \"datavalue\" not in temp:\n",
    "        return True\n",
    "\n",
    "    skip = {\n",
    "        \"wikibase-lexeme\",\n",
    "        \"wikibase-form\",\n",
    "        \"wikibase-sense\"\n",
    "    }\n",
    "\n",
    "    return datatype in skip\n",
    "\n",
    "\n",
    "def get_value(obj, datatype):\n",
    "    temp = obj.get(\"mainsnak\", obj)\n",
    "    if datatype == \"globe-coordinate\":\n",
    "        latitude = temp[\"datavalue\"][\"value\"][\"latitude\"]\n",
    "        longitude = temp[\"datavalue\"][\"value\"][\"longitude\"]\n",
    "        value = f\"{latitude},{longitude}\"\n",
    "    else:\n",
    "        keys = {\n",
    "            \"quantity\": \"amount\",\n",
    "            \"monolingualtext\": \"text\",\n",
    "            \"time\": \"time\",\n",
    "        }\n",
    "        if datatype in keys:\n",
    "            key = keys[datatype]\n",
    "            value = temp[\"datavalue\"][\"value\"][key]\n",
    "        else:\n",
    "            value = temp[\"datavalue\"][\"value\"]\n",
    "    return value\n",
    "\n",
    "global initial_total_lines_estimate\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    organization_subclass = []\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = []\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = []\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = []\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = []\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = []\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = []\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = []\n",
    "\n",
    "# Removing overlaps for organization_subclass\n",
    "organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    geolocation_subclass = []\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = []\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = []\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = []\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = []\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = []\n",
    "\n",
    "# Removing overlaps for geolocation_subclass\n",
    "geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "\n",
    "\n",
    "\n",
    "with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "    count = 1000\n",
    "    \n",
    "    ORG = []\n",
    "    PERS = []\n",
    "    LOC = []\n",
    "    OTHERS = []\n",
    "\n",
    "    pbar = tqdm(total=initial_total_lines_estimate)\n",
    "\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            # Parse JSON data from each line\n",
    "            item = json.loads(line[:-2])\n",
    "\n",
    "            entity = item['id']\n",
    "            labels = item.get(\"labels\", {})\n",
    "            english_label = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "            aliases = item.get(\"aliases\", {})\n",
    "            description = item.get('descriptions', {}).get('en', {})\n",
    "            category = \"entity\"\n",
    "            sitelinks = item.get(\"sitelinks\", {})\n",
    "            popularity = len(sitelinks) if len(sitelinks) > 0 else 1\n",
    "   \n",
    "            print(f\"{english_label} - {entity}\")\n",
    "\n",
    "            if entity in list(mapping.values()):\n",
    "                all_labels = {}\n",
    "                for lang in labels:\n",
    "                    all_labels[lang] = labels[lang][\"value\"]\n",
    "            \n",
    "                all_aliases = {}\n",
    "                for lang in aliases:\n",
    "                    all_aliases[lang] = []\n",
    "                    for alias in aliases[lang]:\n",
    "                        all_aliases[lang].append(alias[\"value\"])\n",
    "                    all_aliases[lang] = list(set(all_aliases[lang]))\n",
    "            \n",
    "                for predicate in item[\"claims\"]:\n",
    "                    if predicate == \"P279\":\n",
    "                        category = \"type\"\n",
    "                        break\n",
    "                    if predicate == \"P31\":\n",
    "                        if 'Q4167410' == entity:\n",
    "                            category = \"disambiguation\"\n",
    "                            break\n",
    "                        elif 'Q4167836' == entity:\n",
    "                            category = \"category\"\n",
    "                            break\n",
    "\n",
    "                if entity[0] == \"P\":\n",
    "                    category = \"predicate\"\n",
    "        \n",
    "                line_size = len(line)\n",
    "                current_average_size = update_average_size(line_size)\n",
    "                pbar.total = round(compressed_file_size / current_average_size)\n",
    "                pbar.update(1)\n",
    "    \n",
    "                ###############################################################\n",
    "                # ORGANIZATION EXTRACTION\n",
    "                # All items with the root class Organization (Q43229) excluding country (Q6256), city (Q515), capitals (Q5119), \n",
    "                # administrative territorial entity of a single country (Q15916867), venue (Q17350442), sports league (Q623109) \n",
    "                # and family (Q8436)\n",
    "                \n",
    "                # LOCATION EXTRACTION\n",
    "                # All items with the root class Geographic Location (Q2221906) excluding: food (Q2095), educational institution (Q2385804), \n",
    "                # government agency (Q327333), international organization (Q484652) and time zone (Q12143)\n",
    "                \n",
    "                # PERSON EXTRACTION\n",
    "                # All items with the statement is instance of (P31) human (Q5) are classiﬁed as person.\n",
    "    \n",
    "                NERtype = None\n",
    "    \n",
    "                if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "                    p31_claims = item[\"claims\"].get(\"P31\", [])\n",
    "                    ner_counter = Counter()\n",
    "                    \n",
    "                    if len(p31_claims) != 0:           \n",
    "                        for claim in p31_claims:\n",
    "                            mainsnak = claim.get(\"mainsnak\", {})\n",
    "                            datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                            numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "                            \n",
    "                            # Classify NER types\n",
    "                            if numeric_id == 5:\n",
    "                                ner_counter['PERS'] += 1\n",
    "                            elif numeric_id in geolocation_subclass:\n",
    "                                ner_counter['LOC'] += 1\n",
    "                            elif numeric_id in organization_subclass:\n",
    "                                ner_counter['ORG'] += 1\n",
    "                            else:\n",
    "                                ner_counter['OTHERS'] += 1\n",
    "                                \n",
    "                        # Add numeric_id to all NER categories it belongs to\n",
    "\n",
    "                        for ner_type in ner_counter:\n",
    "                            if ner_type == 'ORG':\n",
    "                                print(f\"--> {ner_type} \")\n",
    "                                ORG.append(numeric_id)\n",
    "                            elif ner_type == 'PERS':\n",
    "                                print(f\"--> {ner_type} \")\n",
    "                                PERS.append(numeric_id)\n",
    "                            elif ner_type == 'LOC':\n",
    "                                print(f\"--> {ner_type} \")\n",
    "                                LOC.append(numeric_id)\n",
    "                            elif ner_type == 'OTHERS':\n",
    "                                print(f\"--> {ner_type} \")\n",
    "                                OTHERS.append(numeric_id)\n",
    "\n",
    "                        \n",
    "                ################################################################   \n",
    "                # TRANSITIVE CLOSURE\n",
    "\n",
    "                if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "                    p31_claims = item[\"claims\"].get(\"P31\", [])\n",
    "\n",
    "                    types_list = []\n",
    "\n",
    "                    for claim in p31_claims:\n",
    "                        mainsnak = claim.get(\"mainsnak\", {})\n",
    "                        datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                        type_numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "                        types_list.append(\"Q\"+str(type_numeric_id))\n",
    "\n",
    "                total = []\n",
    "                for el in types_list:\n",
    "                    total += retrieve_superclasses(el)\n",
    "                    superclasses = retrieve_superclasses(el)  # Replace with your entity ID\n",
    "                    print(f\"[{el}] - # superclasses: {len(superclasses)}\")\n",
    "                print(f\"Superclasses: [{len(set(total))}] {set(total)}\")\n",
    "                print(\"_______________________\")\n",
    "\n",
    "                ################################################################   \n",
    "                # URL EXTRACTION\n",
    "            \n",
    "                try:\n",
    "                    lang = labels.get(\"en\", {}).get(\"language\", \"\")\n",
    "                    tmp={}\n",
    "                    tmp[\"WD_id\"] = item['id']\n",
    "                    tmp[\"WP_id\"] = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "            \n",
    "                    url_dict={}\n",
    "                    url_dict[\"wikidata\"] = \"http://www.wikidata.org/wiki/\"+tmp[\"WD_id\"]\n",
    "                    url_dict[\"wikipedia\"] = \"http://\"+lang+\".wikipedia.org/wiki/\"+sitelinks['enwiki']['title'].replace(' ','_')\n",
    "                    url_dict[\"dbpedia\"] = \"http://dbpedia.org/resource/\"+sitelinks['enwiki']['title'].replace(' ','_')\n",
    "                    \n",
    "            \n",
    "                except:\n",
    "                   pass\n",
    "                \n",
    "                ################################################################    \n",
    "        \n",
    "                objects = {}\n",
    "                literals = {datatype: {} for datatype in DATATYPES}\n",
    "                types = {\"P31\": []}\n",
    "                join = {\n",
    "                    \"items\": {\n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"description\": description,\n",
    "                        \"labels\": all_labels,\n",
    "                        \"aliases\": all_aliases,\n",
    "                        \"types\": types,\n",
    "                        \"popularity\": popularity,\n",
    "                        \"kind\": category,   # kind (entity, type or predicate, disambiguation or category)\n",
    "                        ######################\n",
    "                        # new updates\n",
    "                        \"NERtype\": NERtype, # (ORG, LOC, PER or OTHERS)\n",
    "                        \"URLs\" : url_dict\n",
    "                        ######################\n",
    "                    },\n",
    "                    \"objects\": { \n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"objects\":objects\n",
    "                    },\n",
    "                    \"literals\": { \n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"literals\": literals\n",
    "                    },\n",
    "                    \"types\": { \n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"types\": types\n",
    "                    },\n",
    "                }\n",
    "            \n",
    "                predicates = item[\"claims\"]\n",
    "                for predicate in predicates:\n",
    "                    for obj in predicates[predicate]:\n",
    "                        datatype = obj[\"mainsnak\"][\"datatype\"]\n",
    "            \n",
    "                        if check_skip(obj, datatype):\n",
    "                            continue\n",
    "            \n",
    "                        if datatype == \"wikibase-item\" or datatype == \"wikibase-property\":\n",
    "                            value = obj[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "            \n",
    "                            if predicate == \"P31\" or predicate == \"P106\":\n",
    "                                types[\"P31\"].append(value)\n",
    "            \n",
    "                            if value not in objects:\n",
    "                                objects[value] = []\n",
    "                            objects[value].append(predicate)    \n",
    "                        else:\n",
    "                            value = get_value(obj, datatype)                \n",
    "                            lit = literals[DATATYPES_MAPPINGS[datatype]]\n",
    "            \n",
    "                            if predicate not in lit:\n",
    "                                lit[predicate] = []\n",
    "                            lit[predicate].append(value)   \n",
    "            \n",
    "                 \n",
    "            \n",
    "                for key in buffer:\n",
    "                    buffer[key].append(join[key])            \n",
    "            \n",
    "                if len(buffer[\"items\"]) == BATCH_SIZE:\n",
    "                    flush_buffer(buffer)\n",
    "    \n",
    "        except json.decoder.JSONDecodeError:\n",
    "            continue\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a471b54-5f11-41dc-ab4f-68c940dc7c92",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fulda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[43menglish_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfulda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fulda'"
     ]
    }
   ],
   "source": [
    "(english_labels['fulda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a06f6-0932-4c54-9abc-714df8f1709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./yago_wiki_classification.json\"\n",
    "\n",
    "data = {\n",
    "    \"ORG\": ORG,\n",
    "    \"LOC\": LOC,\n",
    "    \"PERS\": PERS,\n",
    "    \"OTHERS\": OTHERS\n",
    "}\n",
    "\n",
    "# Write the categorized data to a JSON file\n",
    "try:\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Data saved successfully to {json_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6043e2-23dc-4a5d-8f49-9c8f6b598523",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = parser.parse_all_logs(log_dir=\"./\")\n",
    "first_log = logs[0]\n",
    "\n",
    "print(f\"Output file name: {first_log['output_filename']}\")\n",
    "print(f\"Standard file name: {first_log['standard_filename']}\")\n",
    "print(f\"Stopped early: {first_log['early_stop']}\")\n",
    "print(f\"Measured consumption: {first_log['actual']}\")\n",
    "print(f\"Predicted consumption: {first_log['pred']}\")\n",
    "print(f\"Measured GPU devices: {first_log['components']['gpu']['devices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a850f-8f23-4096-9a22-e594d6ece098",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length_PERS = len(PERS)\n",
    "total_length_ORG = len(ORG)\n",
    "total_length_LOC = len(LOC)\n",
    "total_length_OTHERS = len(OTHERS)\n",
    "\n",
    "# Print the total lengths\n",
    "print(\"Total lengths:\")\n",
    "print(f\"Length of PERS: {total_length_PERS}\")\n",
    "print(f\"Length of ORG: {total_length_ORG}\")\n",
    "print(f\"Length of LOC: {total_length_LOC}\")\n",
    "print(f\"Length of OTHERS: {total_length_OTHERS}\")\n",
    "\n",
    "# Calculate the sum of lengths\n",
    "total_length = total_length_PERS + total_length_ORG + total_length_LOC + total_length_OTHERS\n",
    "\n",
    "# Print the sum of lengths\n",
    "print(f\"Total length: {total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a9fd3-190e-43d4-93a6-acb0d79af0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in OTHERS:\n",
    "    if el in PERS:\n",
    "        print(f\"PERS and ORG --> Entity ID: {PERS.index(el)}\")\n",
    "    if el in LOC:\n",
    "        print(f\"LOC and ORG --> Entity ID: {LOC.index(el)}\")\n",
    "    if el in ORG:\n",
    "        print(f\"OTHERS and ORG --> Entity ID: {ORG.index(el)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1273d4-71d2-4e5d-9100-48ede8cc4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to sets for faster intersection operation\n",
    "ORG_set = set(ORG)\n",
    "PERS_set = set(PERS)\n",
    "LOC_set = set(LOC)\n",
    "OTHERS_set = set(OTHERS)\n",
    "\n",
    "# Initialize counters for each set\n",
    "ORG_counter = 0\n",
    "PERS_counter = 0\n",
    "LOC_counter = 0\n",
    "OTHERS_counter = 0\n",
    "\n",
    "# Find the overlapping items and update the counters\n",
    "for item in ORG_set.union(PERS_set, LOC_set, OTHERS_set):\n",
    "    num_overlaps = 0\n",
    "    if item in ORG_set:\n",
    "        print(\"item\")\n",
    "        num_overlaps += 1\n",
    "    if item in PERS_set:\n",
    "        num_overlaps += 1\n",
    "    if item in LOC_set:\n",
    "        num_overlaps += 1\n",
    "    if item in OTHERS_set:\n",
    "        num_overlaps += 1\n",
    "    \n",
    "    # Update the corresponding counter based on the number of overlaps\n",
    "    if num_overlaps == 1:\n",
    "        ORG_counter += 1\n",
    "    elif num_overlaps == 2:\n",
    "        PERS_counter += 1\n",
    "    elif num_overlaps == 3:\n",
    "        LOC_counter += 1\n",
    "    elif num_overlaps == 4:\n",
    "        OTHERS_counter += 1\n",
    "\n",
    "# Print the counts for each set\n",
    "print(\"Number of overlaps for each set:\")\n",
    "print(f\"ORG: {ORG_counter}\")\n",
    "print(f\"PERS: {PERS_counter}\")\n",
    "print(f\"LOC: {LOC_counter}\")\n",
    "print(f\"OTHERS: {OTHERS_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c9245",
   "metadata": {},
   "source": [
    "## URL Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4029e46-84a4-4177-a31b-e228d4149814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# This Python file uses the following encoding: utf-8\n",
    "\n",
    "__author__ = 'jgeiss'\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# authors: Johanna Geiß, Heidelberg University, Germany                     #\n",
    "# email: geiss@informatik.uni-heidelberg.de                                 #\n",
    "# Copyright (c) 2017 Database Research Group,                               #\n",
    "#               Institute of Computer Science,                              #\n",
    "#               University of Heidelberg                                    #\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");         #\n",
    "#   you may not use this file except in compliance with the License.        #\n",
    "#   You may obtain a copy of the License at                                 #\n",
    "#                                                                           #\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0                              #\n",
    "#                                                                           #\n",
    "#   Unless required by applicable law or agreed to in writing, software     #\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,       #\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.#\n",
    "#   See the License for the specific language governing permissions and     #\n",
    "#   limitations under the License.                                          #\n",
    "#############################################################################\n",
    "# last updated 21.3.2017 by Johanna Geiß\n",
    "\n",
    "from pymongo import *\n",
    "from pymongo import errors\n",
    "import configparser\n",
    "\n",
    "\n",
    "\n",
    "wikidata_dump_path = './my-data/latest-all.json.bz2'\n",
    "\n",
    "with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "    count = 0\n",
    "    \n",
    "             \n",
    "    for i, line in tqdm(enumerate(f), total=1000):\n",
    "        if count == 10000:\n",
    "            break\n",
    "        try:\n",
    "            count += 1\n",
    "            # Parse JSON data from each line\n",
    "            data = json.loads(line[:-2])\n",
    "         \n",
    "            labels = data.get(\"labels\", {})\n",
    "            lang = labels.get(\"en\", {}).get(\"language\", \"\")\n",
    "            entry={}\n",
    "            entry[\"WD_id\"] = data['id']\n",
    "            entry[\"WP_id\"] = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "\n",
    "            entry[\"WD_id_URL\"] = \"http://www.wikidata.org/wiki/\"+entry[\"WD_id\"]\n",
    "            entry[\"WP_id_URL\"] = \"http://\"+lang+\".wikipedia.org/wiki/\"+entry[\"WP_id\"].replace(\" \",\"_\")\n",
    "            entry[\"dbpedia_URL\"] = \"http://dbpedia.org/resource/\"+entry[\"WP_id\"].capitalize().replace(\" \",\"_\")\n",
    "            \n",
    "            print(\"------------------\")\n",
    "            print(entry[\"WD_id_URL\"])\n",
    "            print(entry[\"WP_id_URL\"])\n",
    "            print(entry[\"dbpedia_URL\"])\n",
    "            print(\"------------------\")\n",
    "    \n",
    "        except json.decoder.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4405d-3f61-4355-ac23-5e685e372807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbontracker import parser\n",
    "\n",
    "logs = parser.parse_all_logs(log_dir=\"./\")\n",
    "print(logs)\n",
    "first_log = logs[0]\n",
    "\n",
    "print(f\"Output file name: {first_log['output_filename']}\")\n",
    "print(f\"Standard file name: {first_log['standard_filename']}\")\n",
    "print(f\"Stopped early: {first_log['early_stop']}\")\n",
    "print(f\"Measured consumption: {first_log['actual']}\")\n",
    "print(f\"Predicted consumption: {first_log['pred']}\")\n",
    "print(f\"Measured GPU devices: {first_log['components']['gpu']['devices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c39bc5-a679-46c2-8406-0726fc6737cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

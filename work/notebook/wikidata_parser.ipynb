{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497013ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SPARQLWrapper in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: rdflib>=6.1.1 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from SPARQLWrapper) (7.0.0)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.6.1)\n",
      "Requirement already satisfied: six in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c242f0f9-c9e3-4f3b-8d15-96050eeeb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo import *\n",
    "from pymongo import errors\n",
    "import configparser\n",
    "from json.decoder import JSONDecodeError\n",
    "from collections import Counter\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import time\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef38472-3087-45f2-ae9b-fdd8757076a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['mongo:27017'], document_class=dict, tz_aware=False, connect=True)\n"
     ]
    }
   ],
   "source": [
    "# MongoDB connection setup\n",
    "MONGO_ENDPOINT, MONGO_ENDPOINT_PORT = os.environ[\"MONGO_ENDPOINT\"].split(\":\")\n",
    "MONGO_ENDPOINT_PORT = int(MONGO_ENDPOINT_PORT)\n",
    "MONGO_ENDPOINT_USERNAME = os.environ[\"MONGO_INITDB_ROOT_USERNAME\"]\n",
    "MONGO_ENDPOINT_PASSWORD = os.environ[\"MONGO_INITDB_ROOT_PASSWORD\"]\n",
    "DB_NAME = f\"wikidata\"\n",
    "\n",
    "client = MongoClient(MONGO_ENDPOINT, MONGO_ENDPOINT_PORT, username=MONGO_ENDPOINT_USERNAME, password=MONGO_ENDPOINT_PASSWORD)\n",
    "print(client)\n",
    "\n",
    "log_c = client.wikidata.log\n",
    "items_c = client[DB_NAME].items\n",
    "objects_c = client[DB_NAME].objects\n",
    "literals_c = client[DB_NAME].literals\n",
    "types_c = client[DB_NAME].types\n",
    "\n",
    "c_ref = {\n",
    "    \"items\": items_c,\n",
    "    \"objects\":objects_c, \n",
    "    \"literals\":literals_c, \n",
    "    \"types\":types_c\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16daa576-49bf-4fd5-960f-a034684916a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_buffer(buffer):\n",
    "    for key in buffer:\n",
    "        if len(buffer[key]) > 0:\n",
    "            c_ref[key].insert_many(buffer[key])\n",
    "            buffer[key] = []\n",
    "\n",
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ccbd4c-be3e-4b23-a8b8-d93e3f9397fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/def_mapping.json\"\n",
    "\n",
    "try:\n",
    "    # Open the JSON file for reading\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        mapping = json.load(json_file)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{json_file_path}' not found.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON data: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b855d5-e433-489e-a4fc-136eaba530e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_processed = 0\n",
    "num_entities_processed = 0\n",
    "\n",
    "def update_average_size(new_size):\n",
    "    global total_size_processed, num_entities_processed\n",
    "    total_size_processed += new_size\n",
    "    num_entities_processed += 1\n",
    "    return total_size_processed / num_entities_processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f102de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_superclasses(entity_id):\n",
    "    \"\"\"\n",
    "    Retrieve all superclasses of a given Wikidata entity ID.\n",
    "\n",
    "    Args:\n",
    "        entity_id (str): The ID of the entity (e.g., \"Q207784\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are superclass IDs, and values are their labels.\n",
    "    \"\"\"\n",
    "    # Define the SPARQL endpoint and query\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?superclass ?superclassLabel WHERE {{\n",
    "      wd:{entity_id} (wdt:P279)* ?superclass.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to query the SPARQL endpoint with retries\n",
    "    def query_wikidata(sparql_client, query, retries=3, delay=5):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                sparql_client.setQuery(query)\n",
    "                sparql_client.setReturnFormat(JSON)\n",
    "                results = sparql_client.query().convert()\n",
    "                return results\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):  # Handle Too Many Requests error\n",
    "                    print(f\"Rate limit hit. Retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    break\n",
    "        return None\n",
    "\n",
    "    # Set up the SPARQL client\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "\n",
    "    # Execute the query with retries\n",
    "    results = query_wikidata(sparql, query)\n",
    "\n",
    "    # Process results and return as a dictionary\n",
    "    if results:\n",
    "        superclass_dict = {}\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            superclass_id = result[\"superclass\"][\"value\"].split(\"/\")[-1]  # Extract entity ID from the URI\n",
    "            label = result[\"superclassLabel\"][\"value\"]\n",
    "            superclass_dict[label] = \"Q\"+(superclass_id[1:])\n",
    "        return list(superclass_dict.values())\n",
    "    else:\n",
    "        print(\"Failed to retrieve data after multiple attempts.\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b8114a-6765-4c4c-ab18-1a5c17c8aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/332984 [01:44<1938:52:00, 20.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belgium - Q31\n",
      "--> LOC \n",
      "--> ORG \n",
      "[Q3624078] - # superclasses: 54\n",
      "[Q43702] - # superclasses: 48\n",
      "[Q6256] - # superclasses: 44\n",
      "[Q20181813] - # superclasses: 55\n",
      "[Q185441] - # superclasses: 55\n",
      "[Q1250464] - # superclasses: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q113489728] - # superclasses: 55\n",
      "Superclasses: [65] {'Q16334298', 'Q185441', 'Q131085629', 'Q7210356', 'Q2221906', 'Q20181813', 'Q56061', 'Q123349660', 'Q21157127', 'Q27096213', 'Q4835091', 'Q155076', 'Q43229', 'Q7275', 'Q1063239', 'Q106559804', 'Q82794', 'Q488383', 'Q26713767', 'Q16887380', 'Q28108', 'Q103940464', 'Q106668099', 'Q1048835', 'Q58415929', 'Q1896989', 'Q3624078', 'Q16334295', 'Q211606', 'Q6671777', 'Q27096235', 'Q25404640', 'Q124711467', 'Q3455524', 'Q96196009', 'Q124711484', 'Q1140229', 'Q8191099', 'Q24229398', 'Q1639378', 'Q22676603', 'Q98119401', 'Q15642541', 'Q43702', 'Q61961344', 'Q1646605', 'Q99527517', 'Q3778211', 'Q6256', 'Q177634', 'Q53617489', 'Q58778', 'Q618123', 'Q16562419', 'Q23956024', 'Q16686448', 'Q35120', 'Q178706', 'Q18810687', 'Q1250464', 'Q113489728', 'Q854457', 'Q874405', 'Q53617407', 'Q484652'}\n",
      "types_list: [7] ['Q3624078', 'Q43702', 'Q6256', 'Q20181813', 'Q185441', 'Q1250464', 'Q113489728']\n",
      "_______________________\n",
      "happiness - Q8\n",
      "--> OTHERS \n",
      "[Q331769] - # superclasses: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q60539479] - # superclasses: 17\n",
      "Superclasses: [31] {'Q9415', 'Q7268708', 'Q282250', 'Q12047512', 'Q5127848', 'Q2996394', 'Q813912', 'Q17320256', 'Q1293220', 'Q7048977', 'Q488383', 'Q331769', 'Q58415929', 'Q781413', 'Q64732777', 'Q937228', 'Q30241068', 'Q1190554', 'Q3249551', 'Q3505845', 'Q67518978', 'Q41537118', 'Q12047513', 'Q1322005', 'Q99527517', 'Q483247', 'Q111752858', 'Q35120', 'Q60539479', 'Q20937557', 'Q3968640'}\n",
      "types_list: [2] ['Q331769', 'Q60539479']\n",
      "_______________________\n",
      "George Washington - Q23\n",
      "--> PERS \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q5] - # superclasses: 30\n",
      "Superclasses: [30] {'Q164509', 'Q5', 'Q223557', 'Q5127848', 'Q106559804', 'Q488383', 'Q7048977', 'Q103940464', 'Q110551885', 'Q27043950', 'Q66394244', 'Q26401003', 'Q729', 'Q215627', 'Q21871294', 'Q12898224', 'Q159344', 'Q154954', 'Q795052', 'Q24229398', 'Q98119401', 'Q99527517', 'Q3778211', 'Q72638', 'Q10855152', 'Q53617489', 'Q35120', 'Q4406616', 'Q7239', 'Q53617407'}\n",
      "types_list: [1] ['Q5']\n",
      "_______________________\n",
      "Jack Bauer - Q24\n",
      "--> OTHERS \n",
      "[Q15632617] - # superclasses: 21\n",
      "[Q15773317] - # superclasses: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q20085850] - # superclasses: 12\n",
      "Superclasses: [23] {'Q28020127', 'Q64728693', 'Q15632617', 'Q95074', 'Q30017383', 'Q115537581', 'Q2593744', 'Q96789464', 'Q3542731', 'Q14897293', 'Q7048977', 'Q97498056', 'Q488383', 'Q103940464', 'Q115257598', 'Q24229398', 'Q15773317', 'Q27277631', 'Q20085850', 'Q53617489', 'Q35120', 'Q16686448', 'Q122192387'}\n",
      "types_list: [3] ['Q15632617', 'Q15773317', 'Q20085850']\n",
      "_______________________\n",
      "Douglas Adams - Q42\n",
      "--> PERS \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q5] - # superclasses: 30\n",
      "Superclasses: [30] {'Q164509', 'Q5', 'Q223557', 'Q5127848', 'Q106559804', 'Q488383', 'Q7048977', 'Q103940464', 'Q110551885', 'Q27043950', 'Q66394244', 'Q26401003', 'Q729', 'Q215627', 'Q21871294', 'Q12898224', 'Q159344', 'Q154954', 'Q795052', 'Q24229398', 'Q98119401', 'Q99527517', 'Q3778211', 'Q72638', 'Q10855152', 'Q53617489', 'Q35120', 'Q4406616', 'Q7239', 'Q53617407'}\n",
      "types_list: [1] ['Q5']\n",
      "_______________________\n",
      "Paul Otlet - Q1868\n",
      "--> PERS \n",
      "[Q5] - # superclasses: 30\n",
      "Superclasses: [30] {'Q164509', 'Q5', 'Q223557', 'Q5127848', 'Q106559804', 'Q488383', 'Q7048977', 'Q103940464', 'Q110551885', 'Q27043950', 'Q66394244', 'Q26401003', 'Q729', 'Q215627', 'Q21871294', 'Q12898224', 'Q159344', 'Q154954', 'Q795052', 'Q24229398', 'Q98119401', 'Q99527517', 'Q3778211', 'Q72638', 'Q10855152', 'Q53617489', 'Q35120', 'Q4406616', 'Q7239', 'Q53617407'}\n",
      "types_list: [1] ['Q5']\n",
      "_______________________\n",
      "Wikidata - Q2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 143>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    197\u001b[0m current_average_size \u001b[38;5;241m=\u001b[39m update_average_size(line_size)\n\u001b[0;32m    198\u001b[0m pbar\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(compressed_file_size \u001b[38;5;241m/\u001b[39m current_average_size)\n\u001b[1;32m--> 199\u001b[0m \u001b[43mpbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m###############################################################\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# ORGANIZATION EXTRACTION\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# All items with the root class Organization (Q43229) excluding country (Q6256), city (Q515), capitals (Q5119), \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# PERSON EXTRACTION\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# All items with the statement is instance of (P31) human (Q5) are classiﬁed as person.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m NERtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\tqdm\\std.py:1242\u001b[0m, in \u001b[0;36mtqdm.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\tqdm\\std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\tqdm\\std.py:1494\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TqdmDeprecationWarning(\n\u001b[0;32m   1489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `tqdm.gui.tqdm(...)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instead of `tqdm(..., gui=True)`\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1491\u001b[0m         fp_write\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m'\u001b[39m, sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite))\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m-> 1494\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmoveto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg)\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\tqdm\\std.py:1443\u001b[0m, in \u001b[0;36mtqdm.moveto\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmoveto\u001b[39m(\u001b[38;5;28mself\u001b[39m, n):\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;66;03m# TODO: private method\u001b[39;00m\n\u001b[1;32m-> 1443\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_term_move_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)()\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\tqdm\\utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\ipykernel\\iostream.py:555\u001b[0m, in \u001b[0;36mOutStream.write\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 555\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(string)\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\ipykernel\\iostream.py:461\u001b[0m, in \u001b[0;36mOutStream._schedule_flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_schedule_in_thread\u001b[39m():\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_loop\u001b[38;5;241m.\u001b[39mcall_later(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_interval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[1;32m--> 461\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_schedule_in_thread\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\ipykernel\\iostream.py:210\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m     f()\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\zmq\\sugar\\socket.py:618\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    611\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[0;32m    612\u001b[0m             data,\n\u001b[0;32m    613\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[0;32m    614\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    615\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[0;32m    616\u001b[0m         )\n\u001b[0;32m    617\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m--> 618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:740\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:787\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:244\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wikidata_dump_path = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/latest-all.json.bz2'\n",
    "initial_estimated_average_size = 800\n",
    "BATCH_SIZE = 100 # Number of entities to insert in a single batch\n",
    "compressed_file_size = os.path.getsize(wikidata_dump_path)\n",
    "initial_total_lines_estimate = compressed_file_size / initial_estimated_average_size\n",
    "\n",
    "DATATYPES_MAPPINGS = {\n",
    "    'external-id': 'STRING',\n",
    "    'quantity': 'NUMBER',\n",
    "    'globe-coordinate': 'STRING',\n",
    "    'string': 'STRING',\n",
    "    'monolingualtext': 'STRING',\n",
    "    'commonsMedia': 'STRING',\n",
    "    'time': 'DATETIME',\n",
    "    'url': 'STRING',\n",
    "    'geo-shape': 'GEOSHAPE',\n",
    "    'math': 'MATH',\n",
    "    'musical-notation': 'MUSICAL_NOTATION',\n",
    "    'tabular-data': 'TABULAR_DATA'\n",
    "}\n",
    "DATATYPES = list(set(DATATYPES_MAPPINGS.values()))\n",
    "\n",
    "buffer = {\n",
    "    \"items\": [],\n",
    "    \"objects\": [], \n",
    "    \"literals\": [], \n",
    "    \"types\": []\n",
    "}\n",
    "\n",
    "def check_skip(obj, datatype):\n",
    "    temp = obj.get(\"mainsnak\", obj)\n",
    "    if \"datavalue\" not in temp:\n",
    "        return True\n",
    "\n",
    "    skip = {\n",
    "        \"wikibase-lexeme\",\n",
    "        \"wikibase-form\",\n",
    "        \"wikibase-sense\"\n",
    "    }\n",
    "\n",
    "    return datatype in skip\n",
    "\n",
    "\n",
    "def get_value(obj, datatype):\n",
    "    temp = obj.get(\"mainsnak\", obj)\n",
    "    if datatype == \"globe-coordinate\":\n",
    "        latitude = temp[\"datavalue\"][\"value\"][\"latitude\"]\n",
    "        longitude = temp[\"datavalue\"][\"value\"][\"longitude\"]\n",
    "        value = f\"{latitude},{longitude}\"\n",
    "    else:\n",
    "        keys = {\n",
    "            \"quantity\": \"amount\",\n",
    "            \"monolingualtext\": \"text\",\n",
    "            \"time\": \"time\",\n",
    "        }\n",
    "        if datatype in keys:\n",
    "            key = keys[datatype]\n",
    "            value = temp[\"datavalue\"][\"value\"][key]\n",
    "        else:\n",
    "            value = temp[\"datavalue\"][\"value\"]\n",
    "    return value\n",
    "\n",
    "global initial_total_lines_estimate\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    organization_subclass = []\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = []\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = []\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = []\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = []\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = []\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = []\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = []\n",
    "\n",
    "# Removing overlaps for organization_subclass\n",
    "organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    geolocation_subclass = []\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = []\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = []\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = []\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = []\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = []\n",
    "\n",
    "# Removing overlaps for geolocation_subclass\n",
    "geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "\n",
    "\n",
    "\n",
    "with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "    count = 1000\n",
    "    \n",
    "    ORG = []\n",
    "    PERS = []\n",
    "    LOC = []\n",
    "    OTHERS = []\n",
    "\n",
    "    pbar = tqdm(total=initial_total_lines_estimate)\n",
    "\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            # Parse JSON data from each line\n",
    "            item = json.loads(line[:-2])\n",
    "\n",
    "            entity = item['id']\n",
    "            labels = item.get(\"labels\", {})\n",
    "            english_label = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "            aliases = item.get(\"aliases\", {})\n",
    "            description = item.get('descriptions', {}).get('en', {})\n",
    "            category = \"entity\"\n",
    "            sitelinks = item.get(\"sitelinks\", {})\n",
    "            popularity = len(sitelinks) if len(sitelinks) > 0 else 1\n",
    "   \n",
    "            print(f\"{english_label} - {entity}\")\n",
    "\n",
    "            if entity in list(mapping.values()):\n",
    "                all_labels = {}\n",
    "                for lang in labels:\n",
    "                    all_labels[lang] = labels[lang][\"value\"]\n",
    "            \n",
    "                all_aliases = {}\n",
    "                for lang in aliases:\n",
    "                    all_aliases[lang] = []\n",
    "                    for alias in aliases[lang]:\n",
    "                        all_aliases[lang].append(alias[\"value\"])\n",
    "                    all_aliases[lang] = list(set(all_aliases[lang]))\n",
    "            \n",
    "                for predicate in item[\"claims\"]:\n",
    "                    if predicate == \"P279\":\n",
    "                        category = \"type\"\n",
    "                        break\n",
    "                    if predicate == \"P31\":\n",
    "                        if 'Q4167410' == entity:\n",
    "                            category = \"disambiguation\"\n",
    "                            break\n",
    "                        elif 'Q4167836' == entity:\n",
    "                            category = \"category\"\n",
    "                            break\n",
    "\n",
    "                if entity[0] == \"P\":\n",
    "                    category = \"predicate\"\n",
    "        \n",
    "                line_size = len(line)\n",
    "                current_average_size = update_average_size(line_size)\n",
    "                pbar.total = round(compressed_file_size / current_average_size)\n",
    "                pbar.update(1)\n",
    "    \n",
    "                ###############################################################\n",
    "                # ORGANIZATION EXTRACTION\n",
    "                # All items with the root class Organization (Q43229) excluding country (Q6256), city (Q515), capitals (Q5119), \n",
    "                # administrative territorial entity of a single country (Q15916867), venue (Q17350442), sports league (Q623109) \n",
    "                # and family (Q8436)\n",
    "                \n",
    "                # LOCATION EXTRACTION\n",
    "                # All items with the root class Geographic Location (Q2221906) excluding: food (Q2095), educational institution (Q2385804), \n",
    "                # government agency (Q327333), international organization (Q484652) and time zone (Q12143)\n",
    "                \n",
    "                # PERSON EXTRACTION\n",
    "                # All items with the statement is instance of (P31) human (Q5) are classiﬁed as person.\n",
    "    \n",
    "                NERtype = None\n",
    "    \n",
    "                if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "                    p31_claims = item[\"claims\"].get(\"P31\", [])\n",
    "                    ner_counter = Counter()\n",
    "                    \n",
    "                    if len(p31_claims) != 0:           \n",
    "                        for claim in p31_claims:\n",
    "                            mainsnak = claim.get(\"mainsnak\", {})\n",
    "                            datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                            numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "                            \n",
    "                            # Classify NER types\n",
    "                            if numeric_id == 5:\n",
    "                                ner_counter['PERS'] += 1\n",
    "                            elif numeric_id in geolocation_subclass:\n",
    "                                ner_counter['LOC'] += 1\n",
    "                            elif numeric_id in organization_subclass:\n",
    "                                ner_counter['ORG'] += 1\n",
    "                            else:\n",
    "                                ner_counter['OTHERS'] += 1\n",
    "                                \n",
    "                        # Add numeric_id to all NER categories it belongs to\n",
    "\n",
    "                        for ner_type in ner_counter:\n",
    "                            if ner_type == 'ORG':\n",
    "                                print(f\"--> {ner_type} \")\n",
    "                                ORG.append(numeric_id)\n",
    "                            elif ner_type == 'PERS':\n",
    "                                print(f\"--> {ner_type} \")\n",
    "                                PERS.append(numeric_id)\n",
    "                            elif ner_type == 'LOC':\n",
    "                                print(f\"--> {ner_type} \")\n",
    "                                LOC.append(numeric_id)\n",
    "                            elif ner_type == 'OTHERS':\n",
    "                                print(f\"--> {ner_type} \")\n",
    "                                OTHERS.append(numeric_id)\n",
    "\n",
    "                        \n",
    "                ################################################################   \n",
    "                # TRANSITIVE CLOSURE\n",
    "\n",
    "                if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "                    p31_claims = item[\"claims\"].get(\"P31\", [])\n",
    "\n",
    "                    types_list = []\n",
    "\n",
    "                    for claim in p31_claims:\n",
    "                        mainsnak = claim.get(\"mainsnak\", {})\n",
    "                        datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                        type_numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "                        types_list.append(\"Q\"+str(type_numeric_id))\n",
    "\n",
    "                total = []\n",
    "                for el in types_list:\n",
    "                    total += retrieve_superclasses(el)\n",
    "                    superclasses = retrieve_superclasses(el)  # Replace with your entity ID\n",
    "                    print(f\"[{el}] - # superclasses: {len(superclasses)}\")\n",
    "                print(f\"Superclasses: [{len(set(total))}] {set(total)}\")\n",
    "                \n",
    "                print(f\"types_list: [{len(types_list)}] {types_list}\")\n",
    "                print(\"_______________________\")\n",
    "\n",
    "                ################################################################   \n",
    "                # URL EXTRACTION\n",
    "            \n",
    "                try:\n",
    "                    lang = labels.get(\"en\", {}).get(\"language\", \"\")\n",
    "                    tmp={}\n",
    "                    tmp[\"WD_id\"] = item['id']\n",
    "                    tmp[\"WP_id\"] = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "            \n",
    "                    url_dict={}\n",
    "                    url_dict[\"wikidata\"] = \"http://www.wikidata.org/wiki/\"+tmp[\"WD_id\"]\n",
    "                    url_dict[\"wikipedia\"] = \"http://\"+lang+\".wikipedia.org/wiki/\"+sitelinks['enwiki']['title'].replace(' ','_')\n",
    "                    url_dict[\"dbpedia\"] = \"http://dbpedia.org/resource/\"+sitelinks['enwiki']['title'].replace(' ','_')\n",
    "                    \n",
    "            \n",
    "                except:\n",
    "                   pass\n",
    "                \n",
    "                ################################################################    \n",
    "        \n",
    "                objects = {}\n",
    "                literals = {datatype: {} for datatype in DATATYPES}\n",
    "                types = {\"P31\": []}\n",
    "                join = {\n",
    "                    \"items\": {\n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"description\": description,\n",
    "                        \"labels\": all_labels,\n",
    "                        \"aliases\": all_aliases,\n",
    "                        \"types\": types,\n",
    "                        \"popularity\": popularity,\n",
    "                        \"kind\": category,   # kind (entity, type or predicate, disambiguation or category)\n",
    "                        ######################\n",
    "                        # new updates\n",
    "                        \"NERtype\": NERtype, # (ORG, LOC, PER or OTHERS)\n",
    "                        \"URLs\" : url_dict\n",
    "                        ######################\n",
    "                    },\n",
    "                    \"objects\": { \n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"objects\":objects\n",
    "                    },\n",
    "                    \"literals\": { \n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"literals\": literals\n",
    "                    },\n",
    "                    \"types\": { \n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"types\": types\n",
    "                    },\n",
    "                }\n",
    "            \n",
    "                predicates = item[\"claims\"]\n",
    "                for predicate in predicates:\n",
    "                    for obj in predicates[predicate]:\n",
    "                        datatype = obj[\"mainsnak\"][\"datatype\"]\n",
    "            \n",
    "                        if check_skip(obj, datatype):\n",
    "                            continue\n",
    "            \n",
    "                        if datatype == \"wikibase-item\" or datatype == \"wikibase-property\":\n",
    "                            value = obj[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "            \n",
    "                            if predicate == \"P31\" or predicate == \"P106\":\n",
    "                                types[\"P31\"].append(value)\n",
    "            \n",
    "                            if value not in objects:\n",
    "                                objects[value] = []\n",
    "                            objects[value].append(predicate)    \n",
    "                        else:\n",
    "                            value = get_value(obj, datatype)                \n",
    "                            lit = literals[DATATYPES_MAPPINGS[datatype]]\n",
    "            \n",
    "                            if predicate not in lit:\n",
    "                                lit[predicate] = []\n",
    "                            lit[predicate].append(value)   \n",
    "            \n",
    "                 \n",
    "            \n",
    "                for key in buffer:\n",
    "                    buffer[key].append(join[key])            \n",
    "            \n",
    "                if len(buffer[\"items\"]) == BATCH_SIZE:\n",
    "                    flush_buffer(buffer)\n",
    "    \n",
    "        except json.decoder.JSONDecodeError:\n",
    "            continue\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a471b54-5f11-41dc-ab4f-68c940dc7c92",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fulda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[43menglish_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfulda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fulda'"
     ]
    }
   ],
   "source": [
    "(english_labels['fulda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a06f6-0932-4c54-9abc-714df8f1709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./yago_wiki_classification.json\"\n",
    "\n",
    "data = {\n",
    "    \"ORG\": ORG,\n",
    "    \"LOC\": LOC,\n",
    "    \"PERS\": PERS,\n",
    "    \"OTHERS\": OTHERS\n",
    "}\n",
    "\n",
    "# Write the categorized data to a JSON file\n",
    "try:\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Data saved successfully to {json_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6043e2-23dc-4a5d-8f49-9c8f6b598523",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = parser.parse_all_logs(log_dir=\"./\")\n",
    "first_log = logs[0]\n",
    "\n",
    "print(f\"Output file name: {first_log['output_filename']}\")\n",
    "print(f\"Standard file name: {first_log['standard_filename']}\")\n",
    "print(f\"Stopped early: {first_log['early_stop']}\")\n",
    "print(f\"Measured consumption: {first_log['actual']}\")\n",
    "print(f\"Predicted consumption: {first_log['pred']}\")\n",
    "print(f\"Measured GPU devices: {first_log['components']['gpu']['devices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a850f-8f23-4096-9a22-e594d6ece098",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length_PERS = len(PERS)\n",
    "total_length_ORG = len(ORG)\n",
    "total_length_LOC = len(LOC)\n",
    "total_length_OTHERS = len(OTHERS)\n",
    "\n",
    "# Print the total lengths\n",
    "print(\"Total lengths:\")\n",
    "print(f\"Length of PERS: {total_length_PERS}\")\n",
    "print(f\"Length of ORG: {total_length_ORG}\")\n",
    "print(f\"Length of LOC: {total_length_LOC}\")\n",
    "print(f\"Length of OTHERS: {total_length_OTHERS}\")\n",
    "\n",
    "# Calculate the sum of lengths\n",
    "total_length = total_length_PERS + total_length_ORG + total_length_LOC + total_length_OTHERS\n",
    "\n",
    "# Print the sum of lengths\n",
    "print(f\"Total length: {total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a9fd3-190e-43d4-93a6-acb0d79af0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in OTHERS:\n",
    "    if el in PERS:\n",
    "        print(f\"PERS and ORG --> Entity ID: {PERS.index(el)}\")\n",
    "    if el in LOC:\n",
    "        print(f\"LOC and ORG --> Entity ID: {LOC.index(el)}\")\n",
    "    if el in ORG:\n",
    "        print(f\"OTHERS and ORG --> Entity ID: {ORG.index(el)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1273d4-71d2-4e5d-9100-48ede8cc4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to sets for faster intersection operation\n",
    "ORG_set = set(ORG)\n",
    "PERS_set = set(PERS)\n",
    "LOC_set = set(LOC)\n",
    "OTHERS_set = set(OTHERS)\n",
    "\n",
    "# Initialize counters for each set\n",
    "ORG_counter = 0\n",
    "PERS_counter = 0\n",
    "LOC_counter = 0\n",
    "OTHERS_counter = 0\n",
    "\n",
    "# Find the overlapping items and update the counters\n",
    "for item in ORG_set.union(PERS_set, LOC_set, OTHERS_set):\n",
    "    num_overlaps = 0\n",
    "    if item in ORG_set:\n",
    "        print(\"item\")\n",
    "        num_overlaps += 1\n",
    "    if item in PERS_set:\n",
    "        num_overlaps += 1\n",
    "    if item in LOC_set:\n",
    "        num_overlaps += 1\n",
    "    if item in OTHERS_set:\n",
    "        num_overlaps += 1\n",
    "    \n",
    "    # Update the corresponding counter based on the number of overlaps\n",
    "    if num_overlaps == 1:\n",
    "        ORG_counter += 1\n",
    "    elif num_overlaps == 2:\n",
    "        PERS_counter += 1\n",
    "    elif num_overlaps == 3:\n",
    "        LOC_counter += 1\n",
    "    elif num_overlaps == 4:\n",
    "        OTHERS_counter += 1\n",
    "\n",
    "# Print the counts for each set\n",
    "print(\"Number of overlaps for each set:\")\n",
    "print(f\"ORG: {ORG_counter}\")\n",
    "print(f\"PERS: {PERS_counter}\")\n",
    "print(f\"LOC: {LOC_counter}\")\n",
    "print(f\"OTHERS: {OTHERS_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c9245",
   "metadata": {},
   "source": [
    "## URL Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4029e46-84a4-4177-a31b-e228d4149814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# This Python file uses the following encoding: utf-8\n",
    "\n",
    "__author__ = 'jgeiss'\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# authors: Johanna Geiß, Heidelberg University, Germany                     #\n",
    "# email: geiss@informatik.uni-heidelberg.de                                 #\n",
    "# Copyright (c) 2017 Database Research Group,                               #\n",
    "#               Institute of Computer Science,                              #\n",
    "#               University of Heidelberg                                    #\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");         #\n",
    "#   you may not use this file except in compliance with the License.        #\n",
    "#   You may obtain a copy of the License at                                 #\n",
    "#                                                                           #\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0                              #\n",
    "#                                                                           #\n",
    "#   Unless required by applicable law or agreed to in writing, software     #\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,       #\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.#\n",
    "#   See the License for the specific language governing permissions and     #\n",
    "#   limitations under the License.                                          #\n",
    "#############################################################################\n",
    "# last updated 21.3.2017 by Johanna Geiß\n",
    "\n",
    "from pymongo import *\n",
    "from pymongo import errors\n",
    "import configparser\n",
    "\n",
    "\n",
    "\n",
    "wikidata_dump_path = './my-data/latest-all.json.bz2'\n",
    "\n",
    "with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "    count = 0\n",
    "    \n",
    "             \n",
    "    for i, line in tqdm(enumerate(f), total=1000):\n",
    "        if count == 10000:\n",
    "            break\n",
    "        try:\n",
    "            count += 1\n",
    "            # Parse JSON data from each line\n",
    "            data = json.loads(line[:-2])\n",
    "         \n",
    "            labels = data.get(\"labels\", {})\n",
    "            lang = labels.get(\"en\", {}).get(\"language\", \"\")\n",
    "            entry={}\n",
    "            entry[\"WD_id\"] = data['id']\n",
    "            entry[\"WP_id\"] = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "\n",
    "            entry[\"WD_id_URL\"] = \"http://www.wikidata.org/wiki/\"+entry[\"WD_id\"]\n",
    "            entry[\"WP_id_URL\"] = \"http://\"+lang+\".wikipedia.org/wiki/\"+entry[\"WP_id\"].replace(\" \",\"_\")\n",
    "            entry[\"dbpedia_URL\"] = \"http://dbpedia.org/resource/\"+entry[\"WP_id\"].capitalize().replace(\" \",\"_\")\n",
    "            \n",
    "            print(\"------------------\")\n",
    "            print(entry[\"WD_id_URL\"])\n",
    "            print(entry[\"WP_id_URL\"])\n",
    "            print(entry[\"dbpedia_URL\"])\n",
    "            print(\"------------------\")\n",
    "    \n",
    "        except json.decoder.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4405d-3f61-4355-ac23-5e685e372807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbontracker import parser\n",
    "\n",
    "logs = parser.parse_all_logs(log_dir=\"./\")\n",
    "print(logs)\n",
    "first_log = logs[0]\n",
    "\n",
    "print(f\"Output file name: {first_log['output_filename']}\")\n",
    "print(f\"Standard file name: {first_log['standard_filename']}\")\n",
    "print(f\"Stopped early: {first_log['early_stop']}\")\n",
    "print(f\"Measured consumption: {first_log['actual']}\")\n",
    "print(f\"Predicted consumption: {first_log['pred']}\")\n",
    "print(f\"Measured GPU devices: {first_log['components']['gpu']['devices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c39bc5-a679-46c2-8406-0726fc6737cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
